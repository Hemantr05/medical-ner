{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "BERT-NER-Medical.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "JjPF35vD5KfO",
        "outputId": "047f13c5-d60e-4c86-a397-06434332cc5b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.19.4-py3-none-any.whl (4.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 4.2 MB 5.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.7.1)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.11.4)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.64.0)\n",
            "Collecting pyyaml>=5.1\n",
            "  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n",
            "\u001b[K     |████████████████████████████████| 596 kB 69.3 MB/s \n",
            "\u001b[?25hCollecting tokenizers!=0.11.3,<0.13,>=0.11.1\n",
            "  Downloading tokenizers-0.12.1-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 6.6 MB 56.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Collecting huggingface-hub<1.0,>=0.1.0\n",
            "  Downloading huggingface_hub-0.7.0-py3-none-any.whl (86 kB)\n",
            "\u001b[K     |████████████████████████████████| 86 kB 6.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2022.6.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.6)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (4.2.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.8.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2022.5.18.1)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Installing collected packages: pyyaml, tokenizers, huggingface-hub, transformers\n",
            "  Attempting uninstall: pyyaml\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "Successfully installed huggingface-hub-0.7.0 pyyaml-6.0 tokenizers-0.12.1 transformers-4.19.4\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting datasets\n",
            "  Downloading datasets-2.2.2-py3-none-any.whl (346 kB)\n",
            "\u001b[K     |████████████████████████████████| 346 kB 5.0 MB/s \n",
            "\u001b[?25hCollecting xxhash\n",
            "  Downloading xxhash-3.0.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (212 kB)\n",
            "\u001b[K     |████████████████████████████████| 212 kB 65.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (2.23.0)\n",
            "Collecting dill<0.3.5\n",
            "  Downloading dill-0.3.4-py2.py3-none-any.whl (86 kB)\n",
            "\u001b[K     |████████████████████████████████| 86 kB 4.1 MB/s \n",
            "\u001b[?25hCollecting fsspec[http]>=2021.05.0\n",
            "  Downloading fsspec-2022.5.0-py3-none-any.whl (140 kB)\n",
            "\u001b[K     |████████████████████████████████| 140 kB 69.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pyarrow>=6.0.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (6.0.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from datasets) (21.3)\n",
            "Collecting aiohttp\n",
            "  Downloading aiohttp-3.8.1-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1 MB 71.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from datasets) (1.21.6)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.7/dist-packages (from datasets) (0.70.13)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from datasets) (1.3.5)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.7/dist-packages (from datasets) (4.64.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from datasets) (4.11.4)\n",
            "Collecting responses<0.19\n",
            "  Downloading responses-0.18.0-py3-none-any.whl (38 kB)\n",
            "Requirement already satisfied: huggingface-hub<1.0.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (0.7.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (6.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (3.7.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (4.2.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->datasets) (3.0.9)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (2022.5.18.1)\n",
            "Collecting urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1\n",
            "  Downloading urllib3-1.25.11-py2.py3-none-any.whl (127 kB)\n",
            "\u001b[K     |████████████████████████████████| 127 kB 75.7 MB/s \n",
            "\u001b[?25hCollecting async-timeout<5.0,>=4.0.0a3\n",
            "  Downloading async_timeout-4.0.2-py3-none-any.whl (5.8 kB)\n",
            "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (2.0.12)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (21.4.0)\n",
            "Collecting multidict<7.0,>=4.5\n",
            "  Downloading multidict-6.0.2-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (94 kB)\n",
            "\u001b[K     |████████████████████████████████| 94 kB 4.2 MB/s \n",
            "\u001b[?25hCollecting asynctest==0.13.0\n",
            "  Downloading asynctest-0.13.0-py3-none-any.whl (26 kB)\n",
            "Collecting aiosignal>=1.1.2\n",
            "  Downloading aiosignal-1.2.0-py3-none-any.whl (8.2 kB)\n",
            "Collecting yarl<2.0,>=1.0\n",
            "  Downloading yarl-1.7.2-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (271 kB)\n",
            "\u001b[K     |████████████████████████████████| 271 kB 77.4 MB/s \n",
            "\u001b[?25hCollecting frozenlist>=1.1.1\n",
            "  Downloading frozenlist-1.3.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (144 kB)\n",
            "\u001b[K     |████████████████████████████████| 144 kB 72.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->datasets) (3.8.0)\n",
            "Collecting multiprocess\n",
            "  Downloading multiprocess-0.70.12.2-py37-none-any.whl (112 kB)\n",
            "\u001b[K     |████████████████████████████████| 112 kB 81.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets) (2022.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas->datasets) (1.15.0)\n",
            "Installing collected packages: multidict, frozenlist, yarl, urllib3, asynctest, async-timeout, aiosignal, fsspec, dill, aiohttp, xxhash, responses, multiprocess, datasets\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 1.24.3\n",
            "    Uninstalling urllib3-1.24.3:\n",
            "      Successfully uninstalled urllib3-1.24.3\n",
            "  Attempting uninstall: dill\n",
            "    Found existing installation: dill 0.3.5.1\n",
            "    Uninstalling dill-0.3.5.1:\n",
            "      Successfully uninstalled dill-0.3.5.1\n",
            "  Attempting uninstall: multiprocess\n",
            "    Found existing installation: multiprocess 0.70.13\n",
            "    Uninstalling multiprocess-0.70.13:\n",
            "      Successfully uninstalled multiprocess-0.70.13\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "Successfully installed aiohttp-3.8.1 aiosignal-1.2.0 async-timeout-4.0.2 asynctest-0.13.0 datasets-2.2.2 dill-0.3.4 frozenlist-1.3.0 fsspec-2022.5.0 multidict-6.0.2 multiprocess-0.70.12.2 responses-0.18.0 urllib3-1.25.11 xxhash-3.0.0 yarl-1.7.2\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "dill",
                  "urllib3"
                ]
              }
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pytesseract\n",
            "  Downloading pytesseract-0.3.9-py2.py3-none-any.whl (14 kB)\n",
            "Requirement already satisfied: packaging>=21.3 in /usr/local/lib/python3.7/dist-packages (from pytesseract) (21.3)\n",
            "Collecting Pillow>=8.0.0\n",
            "  Downloading Pillow-9.1.1-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.1 MB 5.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=21.3->pytesseract) (3.0.9)\n",
            "Installing collected packages: Pillow, pytesseract\n",
            "  Attempting uninstall: Pillow\n",
            "    Found existing installation: Pillow 7.1.2\n",
            "    Uninstalling Pillow-7.1.2:\n",
            "      Successfully uninstalled Pillow-7.1.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "albumentations 0.1.12 requires imgaug<0.2.7,>=0.2.5, but you have imgaug 0.2.9 which is incompatible.\u001b[0m\n",
            "Successfully installed Pillow-9.1.1 pytesseract-0.3.9\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "PIL"
                ]
              }
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting seqeval\n",
            "  Downloading seqeval-1.2.2.tar.gz (43 kB)\n",
            "\u001b[K     |████████████████████████████████| 43 kB 1.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.7/dist-packages (from seqeval) (1.21.6)\n",
            "Requirement already satisfied: scikit-learn>=0.21.3 in /usr/local/lib/python3.7/dist-packages (from seqeval) (1.0.2)\n",
            "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.21.3->seqeval) (1.4.1)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.21.3->seqeval) (1.1.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.21.3->seqeval) (3.1.0)\n",
            "Building wheels for collected packages: seqeval\n",
            "  Building wheel for seqeval (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for seqeval: filename=seqeval-1.2.2-py3-none-any.whl size=16180 sha256=e40620528d957b792c777c85e47e1502d825389bfc56cae798dcf6794e4999bf\n",
            "  Stored in directory: /root/.cache/pip/wheels/05/96/ee/7cac4e74f3b19e3158dce26a20a1c86b3533c43ec72a549fd7\n",
            "Successfully built seqeval\n",
            "Installing collected packages: seqeval\n",
            "Successfully installed seqeval-1.2.2\n"
          ]
        }
      ],
      "source": [
        "!pip3 install transformers\n",
        "!pip3 install datasets\n",
        "!pip3 install pytesseract\n",
        "!pip3 install seqeval"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m pip install --upgrade detectron2==0.6 -f \\\n",
        "  https://dl.fbaipublicfiles.com/detectron2/wheels/cu111/torch1.10/index.html"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xu-sBX1x5a_O",
        "outputId": "5d5ba076-4000-4c85-a567-b50b24b5e22d"
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Looking in links: https://dl.fbaipublicfiles.com/detectron2/wheels/cu111/torch1.10/index.html\n",
            "Collecting detectron2==0.6\n",
            "  Downloading https://dl.fbaipublicfiles.com/detectron2/wheels/cu111/torch1.10/detectron2-0.6%2Bcu111-cp37-cp37m-linux_x86_64.whl (7.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 7.0 MB 579 kB/s \n",
            "\u001b[?25hCollecting fvcore<0.1.6,>=0.1.5\n",
            "  Downloading fvcore-0.1.5.post20220512.tar.gz (50 kB)\n",
            "\u001b[K     |████████████████████████████████| 50 kB 3.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: Pillow>=7.1 in /usr/local/lib/python3.7/dist-packages (from detectron2==0.6) (9.1.1)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.7/dist-packages (from detectron2==0.6) (0.8.9)\n",
            "Requirement already satisfied: tensorboard in /usr/local/lib/python3.7/dist-packages (from detectron2==0.6) (2.8.0)\n",
            "Collecting yacs>=0.1.8\n",
            "  Downloading yacs-0.1.8-py3-none-any.whl (14 kB)\n",
            "Requirement already satisfied: pydot in /usr/local/lib/python3.7/dist-packages (from detectron2==0.6) (1.3.0)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from detectron2==0.6) (0.16.0)\n",
            "Collecting omegaconf>=2.1\n",
            "  Downloading omegaconf-2.2.2-py3-none-any.whl (79 kB)\n",
            "\u001b[K     |████████████████████████████████| 79 kB 7.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>4.29.0 in /usr/local/lib/python3.7/dist-packages (from detectron2==0.6) (4.64.0)\n",
            "Requirement already satisfied: pycocotools>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from detectron2==0.6) (2.0.4)\n",
            "Collecting hydra-core>=1.1\n",
            "  Downloading hydra_core-1.2.0-py3-none-any.whl (151 kB)\n",
            "\u001b[K     |████████████████████████████████| 151 kB 46.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: termcolor>=1.1 in /usr/local/lib/python3.7/dist-packages (from detectron2==0.6) (1.1.0)\n",
            "Collecting iopath<0.1.10,>=0.1.7\n",
            "  Downloading iopath-0.1.9-py3-none-any.whl (27 kB)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from detectron2==0.6) (3.2.2)\n",
            "Collecting black==21.4b2\n",
            "  Downloading black-21.4b2-py3-none-any.whl (130 kB)\n",
            "\u001b[K     |████████████████████████████████| 130 kB 67.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: cloudpickle in /usr/local/lib/python3.7/dist-packages (from detectron2==0.6) (1.3.0)\n",
            "Requirement already satisfied: regex>=2020.1.8 in /usr/local/lib/python3.7/dist-packages (from black==21.4b2->detectron2==0.6) (2022.6.2)\n",
            "Collecting pathspec<1,>=0.8.1\n",
            "  Downloading pathspec-0.9.0-py2.py3-none-any.whl (31 kB)\n",
            "Collecting typed-ast>=1.4.2\n",
            "  Downloading typed_ast-1.5.4-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (843 kB)\n",
            "\u001b[K     |████████████████████████████████| 843 kB 65.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=3.7.4 in /usr/local/lib/python3.7/dist-packages (from black==21.4b2->detectron2==0.6) (4.2.0)\n",
            "Requirement already satisfied: click>=7.1.2 in /usr/local/lib/python3.7/dist-packages (from black==21.4b2->detectron2==0.6) (7.1.2)\n",
            "Collecting toml>=0.10.1\n",
            "  Downloading toml-0.10.2-py2.py3-none-any.whl (16 kB)\n",
            "Requirement already satisfied: appdirs in /usr/local/lib/python3.7/dist-packages (from black==21.4b2->detectron2==0.6) (1.4.4)\n",
            "Collecting mypy-extensions>=0.4.3\n",
            "  Downloading mypy_extensions-0.4.3-py2.py3-none-any.whl (4.5 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from fvcore<0.1.6,>=0.1.5->detectron2==0.6) (1.21.6)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from fvcore<0.1.6,>=0.1.5->detectron2==0.6) (6.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from hydra-core>=1.1->detectron2==0.6) (21.3)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.7/dist-packages (from hydra-core>=1.1->detectron2==0.6) (5.7.1)\n",
            "Collecting antlr4-python3-runtime==4.9.*\n",
            "  Downloading antlr4-python3-runtime-4.9.3.tar.gz (117 kB)\n",
            "\u001b[K     |████████████████████████████████| 117 kB 74.1 MB/s \n",
            "\u001b[?25hCollecting portalocker\n",
            "  Downloading portalocker-2.4.0-py2.py3-none-any.whl (16 kB)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->detectron2==0.6) (0.11.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->detectron2==0.6) (1.4.2)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->detectron2==0.6) (3.0.9)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->detectron2==0.6) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.1->matplotlib->detectron2==0.6) (1.15.0)\n",
            "Requirement already satisfied: zipp>=3.1.0 in /usr/local/lib/python3.7/dist-packages (from importlib-resources->hydra-core>=1.1->detectron2==0.6) (3.8.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard->detectron2==0.6) (0.4.6)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorboard->detectron2==0.6) (0.37.1)\n",
            "Requirement already satisfied: protobuf>=3.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard->detectron2==0.6) (3.17.3)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard->detectron2==0.6) (2.23.0)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard->detectron2==0.6) (57.4.0)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard->detectron2==0.6) (1.35.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard->detectron2==0.6) (0.6.1)\n",
            "Requirement already satisfied: grpcio>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard->detectron2==0.6) (1.46.3)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard->detectron2==0.6) (3.3.7)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard->detectron2==0.6) (1.8.1)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard->detectron2==0.6) (1.0.1)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.7/dist-packages (from tensorboard->detectron2==0.6) (1.1.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard->detectron2==0.6) (0.2.8)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard->detectron2==0.6) (4.8)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard->detectron2==0.6) (4.2.4)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard->detectron2==0.6) (1.3.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard->detectron2==0.6) (4.11.4)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard->detectron2==0.6) (0.4.8)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard->detectron2==0.6) (2022.5.18.1)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard->detectron2==0.6) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard->detectron2==0.6) (1.25.11)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard->detectron2==0.6) (3.0.4)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard->detectron2==0.6) (3.2.0)\n",
            "Building wheels for collected packages: fvcore, antlr4-python3-runtime\n",
            "  Building wheel for fvcore (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fvcore: filename=fvcore-0.1.5.post20220512-py3-none-any.whl size=61288 sha256=44d91be8ec848657566706c871d9f77d9684312560a7603a0a6f62f989f9668c\n",
            "  Stored in directory: /root/.cache/pip/wheels/68/20/f9/a11a0dd63f4c13678b2a5ec488e48078756505c7777b75b29e\n",
            "  Building wheel for antlr4-python3-runtime (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for antlr4-python3-runtime: filename=antlr4_python3_runtime-4.9.3-py3-none-any.whl size=144575 sha256=9eabcfb33b0cee39c63f24de3a19f4ed6ded66fa2c2ee1b641f2a5708a098bb0\n",
            "  Stored in directory: /root/.cache/pip/wheels/8b/8d/53/2af8772d9aec614e3fc65e53d4a993ad73c61daa8bbd85a873\n",
            "Successfully built fvcore antlr4-python3-runtime\n",
            "Installing collected packages: portalocker, antlr4-python3-runtime, yacs, typed-ast, toml, pathspec, omegaconf, mypy-extensions, iopath, hydra-core, fvcore, black, detectron2\n",
            "Successfully installed antlr4-python3-runtime-4.9.3 black-21.4b2 detectron2-0.6+cu111 fvcore-0.1.5.post20220512 hydra-core-1.2.0 iopath-0.1.9 mypy-extensions-0.4.3 omegaconf-2.2.2 pathspec-0.9.0 portalocker-2.4.0 toml-0.10.2 typed-ast-1.5.4 yacs-0.1.8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!curl -s https://packagecloud.io/install/repositories/github/git-lfs/script.deb.sh | sudo bash\n",
        "!sudo apt-get install git-lfs\n",
        "!git lfs install"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AfCMB_qD5bBh",
        "outputId": "98c8d17b-90a7-449e-c3fa-5fd2e36200d1"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Detected operating system as Ubuntu/bionic.\n",
            "Checking for curl...\n",
            "Detected curl...\n",
            "Checking for gpg...\n",
            "Detected gpg...\n",
            "Running apt-get update... done.\n",
            "Installing apt-transport-https... done.\n",
            "Installing /etc/apt/sources.list.d/github_git-lfs.list...done.\n",
            "Importing packagecloud gpg key... done.\n",
            "Running apt-get update... done.\n",
            "\n",
            "The repository is setup! You can now install packages.\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "The following package was automatically installed and is no longer required:\n",
            "  libnvidia-common-460\n",
            "Use 'sudo apt autoremove' to remove it.\n",
            "The following packages will be upgraded:\n",
            "  git-lfs\n",
            "1 upgraded, 0 newly installed, 0 to remove and 87 not upgraded.\n",
            "Need to get 7,168 kB of archives.\n",
            "After this operation, 7,962 kB of additional disk space will be used.\n",
            "Get:1 https://packagecloud.io/github/git-lfs/ubuntu bionic/main amd64 git-lfs amd64 3.2.0 [7,168 kB]\n",
            "Fetched 7,168 kB in 1s (10.3 MB/s)\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 76, <> line 1.)\n",
            "debconf: falling back to frontend: Readline\n",
            "debconf: unable to initialize frontend: Readline\n",
            "debconf: (This frontend requires a controlling tty.)\n",
            "debconf: falling back to frontend: Teletype\n",
            "dpkg-preconfigure: unable to re-open stdin: \n",
            "(Reading database ... 155636 files and directories currently installed.)\n",
            "Preparing to unpack .../git-lfs_3.2.0_amd64.deb ...\n",
            "Unpacking git-lfs (3.2.0) over (2.3.4-1) ...\n",
            "Setting up git-lfs (3.2.0) ...\n",
            "Git LFS initialized.\n",
            "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n",
            "Git LFS initialized.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Download dataset"
      ],
      "metadata": {
        "id": "2C0lmT875o63"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import json\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "  \n",
        "from sklearn.metrics import accuracy_score\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import BertTokenizer, BertConfig, BertForTokenClassification\n",
        "\n",
        "from torch import cuda\n",
        "device = 'cuda' if cuda.is_available() else 'cpu'\n",
        "print(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5y2qNXPW5bG5",
        "outputId": "8b54bfb1-9e0f-4dd5-8334-99f24d085533"
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preparing Dataset"
      ],
      "metadata": {
        "id": "fTTyntZDEv0b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset: https://www.kaggle.com/datasets/finalepoch/medical-ner\n",
        "\n",
        "with open('data/annotations.json') as f:\n",
        "  annotations = json.load(f)['examples']"
      ],
      "metadata": {
        "id": "eAzsmITP6Buj"
      },
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert the tag_name(s) in IOB-format\n",
        "# sentence: I have a brain tumour .\n",
        "# tags: 'O', 'O', 'O', 'B-medical_condition', 'I-medical_condition', 'O'\n",
        "\n",
        "dataset = []\n",
        "\n",
        "for idx, data in enumerate(annotations):\n",
        "  sentence_tokens = data['content'].lower().split(\" \")\n",
        "  labels = ['O']*len(sentence_tokens)\n",
        "  entities = data['annotations']\n",
        "  tag_text_pair = {i['value'].lower(): i['tag_name'] for i in entities if i['value'] is not ' '}\n",
        "  for word_idx, word in enumerate(sentence_tokens):\n",
        "    if word in list(tag_text_pair.keys()):\n",
        "      labels[word_idx] = f'B-{tag_text_pair[word]}'\n",
        "  dataset.append({\n",
        "                  'sentence_ids': f'sentence: {idx+1}',\n",
        "                  'sentences_tokens': sentence_tokens,\n",
        "                  'tags': labels\n",
        "                  })"
      ],
      "metadata": {
        "id": "2_ixSOeH5bJw"
      },
      "execution_count": 106,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.DataFrame(dataset)\n",
        "df.head()"
      ],
      "metadata": {
        "id": "d984pjrC5bMH",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "2af2785b-a68b-40d0-a454-1c9b6c993416"
      },
      "execution_count": 107,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "  sentence_ids                                   sentences_tokens  \\\n",
              "0  sentence: 1  [while, bismuth, compounds, (pepto-bismol), de...   \n",
              "1  sentence: 2  [diarrhea,, also, spelled, diarrhoea,, is, the...   \n",
              "2  sentence: 3  [antiretroviral, therapy, (art), is, recommend...   \n",
              "3  sentence: 4  [the, following, drugs, are, considered, as, d...   \n",
              "4  sentence: 5  [the, goals, of, treatment, are, to, reduce, p...   \n",
              "\n",
              "                                                tags  \n",
              "0  [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...  \n",
              "1  [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...  \n",
              "2  [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...  \n",
              "3  [O, O, O, O, O, O, O, O, O, O, O, O, O, O, B-M...  \n",
              "4  [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-406771e6-277e-472f-ae3b-539f5f9ac59b\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sentence_ids</th>\n",
              "      <th>sentences_tokens</th>\n",
              "      <th>tags</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>sentence: 1</td>\n",
              "      <td>[while, bismuth, compounds, (pepto-bismol), de...</td>\n",
              "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>sentence: 2</td>\n",
              "      <td>[diarrhea,, also, spelled, diarrhoea,, is, the...</td>\n",
              "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>sentence: 3</td>\n",
              "      <td>[antiretroviral, therapy, (art), is, recommend...</td>\n",
              "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>sentence: 4</td>\n",
              "      <td>[the, following, drugs, are, considered, as, d...</td>\n",
              "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, B-M...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>sentence: 5</td>\n",
              "      <td>[the, goals, of, treatment, are, to, reduce, p...</td>\n",
              "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-406771e6-277e-472f-ae3b-539f5f9ac59b')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-406771e6-277e-472f-ae3b-539f5f9ac59b button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-406771e6-277e-472f-ae3b-539f5f9ac59b');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 107
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "' '.join(df.sentences_tokens[0])"
      ],
      "metadata": {
        "id": "RN0JzyJ25bO-",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 156
        },
        "outputId": "5ce83a8b-4605-41d2-cf76-efe5f4d8295a"
      },
      "execution_count": 108,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"while bismuth compounds (pepto-bismol) decreased the number of bowel movements in those with travelers' diarrhea, they do not decrease the length of illness.[91] anti-motility agents like loperamide are also effective at reducing the number of stools but not the duration of disease.[8] these agents should be used only if bloody diarrhea is not present.[92]\\n\\ndiosmectite, a natural aluminomagnesium silicate clay, is effective in alleviating symptoms of acute diarrhea in children,[93] and also has some effects in chronic functional diarrhea, radiation-induced diarrhea, and chemotherapy-induced diarrhea.[45] another absorbent agent used for the treatment of mild diarrhea is kaopectate.\\n\\nracecadotril an antisecretory medication may be used to treat diarrhea in children and adults.[86] it has better tolerability than loperamide, as it causes less constipation and flatulence.[94]\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 108
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "','.join(df.tags[0])"
      ],
      "metadata": {
        "id": "b5-wEeG15bRi",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "3f009de2-6b55-484f-8fca-14b4447ad280"
      },
      "execution_count": 109,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,B-Medicine,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,B-MedicalCondition,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,B-MedicalCondition,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,B-MedicalCondition,O,O,O,O,O,O,O,O,O,O,B-MedicalCondition,O,O,O,O,O,O,O,O,O,O,O,O,O,O,B-MedicalCondition,O,O'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 109
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "first_row = df['tags'][0].copy()\n",
        "print(first_row)\n",
        "\n",
        "all_tags = first_row\n",
        "for i in df['tags'][1:]:\n",
        "  all_tags.extend(i)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "58pW_3bCFjBJ",
        "outputId": "e41f627f-aeee-402d-d927-e7c744f1e506"
      },
      "execution_count": 110,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-Medicine', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-MedicalCondition', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-MedicalCondition', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-MedicalCondition', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-MedicalCondition', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-MedicalCondition', 'O', 'O']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('Total of entities: {} \\nNumber of unique entities: {}'.format(len(all_tags), len(set(all_tags))))\n",
        "print('Unique entities: {}'.format(set(all_tags)))\n",
        "\n",
        "unique_entities = list(set(all_tags))\n",
        "print(unique_entities)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S6-gtc2kFpfp",
        "outputId": "d9768da0-32fb-4e7f-eb27-42ec9cb820bd"
      },
      "execution_count": 111,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total of entities: 3114 \n",
            "Number of unique entities: 4\n",
            "Unique entities: {'O', 'B-Pathogen', 'B-Medicine', 'B-MedicalCondition'}\n",
            "['O', 'B-Pathogen', 'B-Medicine', 'B-MedicalCondition']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "label2id = {label:id for id, label in enumerate(unique_entities)}\n",
        "id2label = {id:label for id, label in enumerate(unique_entities)}\n",
        "label2id"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Htj-3Kf-Fy4p",
        "outputId": "38a1fbeb-9f5b-4430-9400-451c6d2b23ce"
      },
      "execution_count": 112,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'B-MedicalCondition': 3, 'B-Medicine': 2, 'B-Pathogen': 1, 'O': 0}"
            ]
          },
          "metadata": {},
          "execution_count": 112
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Hyperparameters**"
      ],
      "metadata": {
        "id": "6NrTnYInEsDk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "MAX_LEN = 128\n",
        "TRAIN_BATCH_SIZE = 2\n",
        "VALID_BATCH_SIZE = 1\n",
        "EPOCHS = 1\n",
        "LEARNING_RATE = 1e-5\n",
        "MAX_GRAD_NORM = 10 # What is the use of this hyperparameter?\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
      ],
      "metadata": {
        "id": "mva7a8yQ5bT8"
      },
      "execution_count": 162,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Helper function**"
      ],
      "metadata": {
        "id": "2YenJKPKEovi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize_and_preserve_labels(sentence, text_labels, tokenizer):\n",
        "  \"\"\"\n",
        "  Word piece tokenization makes it difficult to match word labels\n",
        "  back up with individual word pieces. This function tokenizes each\n",
        "  word one at a time so that it is easier to preserve the correct\n",
        "  label for each subword. It is, ofcourse, a bit slower in processing\n",
        "  time, but it will help our model achieve higher accuracy\n",
        "  \"\"\"\n",
        "  tokenized_sentence = []\n",
        "  labels = []\n",
        "\n",
        "  # sentence = sentence.strip()\n",
        "\n",
        "  for word, label in zip(sentence, text_labels):\n",
        "\n",
        "    # Tokenize the word and count # of subwords the word is broken into\n",
        "    tokenized_word = tokenizer.tokenize(word)\n",
        "    n_subwords = len(tokenized_word)\n",
        "\n",
        "    # Add the tokenized word to the final tokenized word list\n",
        "    tokenized_sentence.extend(tokenized_word)\n",
        "\n",
        "    # Add the same label to the new list of labels `n_subwords` times\n",
        "    labels.extend([label] * n_subwords)\n",
        "\n",
        "  return tokenized_sentence, labels"
      ],
      "metadata": {
        "id": "iOOOZkHz5bXB"
      },
      "execution_count": 163,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Creating DataLoaders"
      ],
      "metadata": {
        "id": "BFRoHecpE1tx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class dataset(Dataset):\n",
        "  def __init__(self, dataframe, tokenizer, max_len):\n",
        "    self.len = len(dataframe)\n",
        "    self.data = dataframe\n",
        "    self.tokenizer = tokenizer\n",
        "    self.max_len = max_len\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "    # step 1: tokenize (and adapt corresponding labels)\n",
        "    # print(\"# step 1: tokenize (and adapt corresponding labels)\")\n",
        "    sentence = self.data.sentences_tokens[index]\n",
        "    # print(\"# of sentence: {}\".format(len(sentence)))\n",
        "    word_labels = self.data.tags[index]\n",
        "    # print(\"# of labels: {}\".format(len(word_labels)))\n",
        "\n",
        "    tokenized_sentence, labels = tokenize_and_preserve_labels(sentence, word_labels, self.tokenizer)\n",
        "\n",
        "    # step 2: add special tokens (and corresponding labels)\n",
        "    # print(\"# step 2: add special tokens (and corresponding labels)\")\n",
        "    tokenized_sentence = [\"[CLS]\"] + tokenized_sentence + [\"[SEP]\"]\n",
        "    labels.insert(0, \"O\") # add outside label for [CLS] token\n",
        "    labels.insert(-1, \"O\") # add outside label for [SEP] token\n",
        "    # print(\"Labels: {}\".format(labels[1]))\n",
        "    # print(\"Labels: {}\".format(labels[0][1]))\n",
        "\n",
        "    # step 3: truncating/padding\n",
        "    # print(\"# step 3: truncating/padding\")\n",
        "    maxlen = self.max_len\n",
        "\n",
        "    if len(tokenized_sentence) > maxlen:\n",
        "      # truncate\n",
        "      tokenized_sentence = tokenized_sentence[:maxlen]\n",
        "      labels = labels[:maxlen]\n",
        "    else:\n",
        "      # pad\n",
        "      tokenized_sentence = tokenized_sentence + ['[PAD]' for _ in range(maxlen - len(tokenized_sentence))]\n",
        "      labels = labels + ['O' for _ in range(maxlen - len(labels))]\n",
        "\n",
        "    # print(\"Labels: {}\".format(labels))\n",
        "\n",
        "    # step 4: obtain the attention mask\n",
        "    attn_mask = [1 if tok != '[PAD]' else 0 for tok in tokenized_sentence]\n",
        "\n",
        "    # step 5: convert tokens to input ids\n",
        "    # print(\"# step 5: convert tokens to input ids\")\n",
        "    ids = self.tokenizer.convert_tokens_to_ids(tokenized_sentence)\n",
        "\n",
        "    label_ids = [label2id[label] for label in labels]\n",
        "    # print(\"Labels: {}\".format(label_ids))\n",
        "\n",
        "    return {\n",
        "        'ids': torch.tensor(ids, dtype=torch.long),\n",
        "        'mask': torch.tensor(attn_mask, dtype=torch.long),\n",
        "        'targets': torch.tensor(label_ids, dtype=torch.long)\n",
        "    }\n",
        "\n",
        "  def __len__(self):\n",
        "    return self.len"
      ],
      "metadata": {
        "id": "_HnNF_hm5bcW"
      },
      "execution_count": 164,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_size = 0.75\n",
        "train_dataset = df.sample(frac=train_size,random_state=200)\n",
        "test_dataset = df.drop(train_dataset.index).reset_index(drop=True)\n",
        "train_dataset = train_dataset.reset_index(drop=True)\n",
        "\n",
        "print(\"FULL Dataset: {}\".format(df.shape))\n",
        "print(\"TRAIN Dataset: {}\".format(train_dataset.shape))\n",
        "print(\"TEST Dataset: {}\".format(test_dataset.shape))\n",
        "\n",
        "training_set = dataset(train_dataset, tokenizer, MAX_LEN)\n",
        "testing_set = dataset(test_dataset, tokenizer, MAX_LEN)\n"
      ],
      "metadata": {
        "id": "U0WyhVne5be0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "57b3456a-fece-43a3-9922-08712bfaca67"
      },
      "execution_count": 165,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "FULL Dataset: (31, 3)\n",
            "TRAIN Dataset: (23, 3)\n",
            "TEST Dataset: (8, 3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "training_set[0]"
      ],
      "metadata": {
        "id": "oVmAPyRb5bhK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "653de873-8527-4124-cd67-c675efd0f321"
      },
      "execution_count": 166,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'ids': tensor([  101,  4973,  1997,  2691,  2529,  7870,  3303,  2011, 18191,  2421,\n",
              "          1996,  2691,  3147,  1010, 24442,  1010,  7975,  6873,  2595,  1010,\n",
              "          1998,  3147, 14699,  2015,  1012,  2116,  3809,  7870,  2107,  2004,\n",
              "         10958, 20536,  1010,  1041, 24290,  7865,  4295,  1010,  8387,  1006,\n",
              "          9820,  1007,  1010, 20704,  2937, 24442,  1010,  1998, 18906,  2015,\n",
              "          2024,  3303,  2011, 18191,  1012,  1996,  5816,  3754,  1997, 18191,\n",
              "          2000,  3426,  4295,  2003,  2649,  1999,  3408,  1997,  6819,  6820,\n",
              "         22717,  1012,  2060,  7870,  2024,  2104,  4812,  2000,  7523,  2065,\n",
              "          2027,  2031,  1037,  7865,  2004,  1996,  6187, 10383,  6024,  4005,\n",
              "          1010,  2107,  2004,  1996,  2825,  4434,  2090,  2529,  2014, 10374,\n",
              "         23350,  1020,  1006,  1044,  2232,  2615,  2575,  1007,  1998, 23130,\n",
              "          7870,  2107,  2004,  3674,  8040,  3917, 12650,  1998, 11888, 16342,\n",
              "          8715,  1012,  1031, 16528,  1033,  2045,  2003,  6704]),\n",
              " 'mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "         1, 1, 1, 1, 1, 1, 1, 1]),\n",
              " 'targets': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0])}"
            ]
          },
          "metadata": {},
          "execution_count": 166
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "training_set[0]['ids']"
      ],
      "metadata": {
        "id": "aH6L7OrQ5bkT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d01ae72e-2522-4253-a086-d553017fa4ed"
      },
      "execution_count": 167,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([  101,  4973,  1997,  2691,  2529,  7870,  3303,  2011, 18191,  2421,\n",
              "         1996,  2691,  3147,  1010, 24442,  1010,  7975,  6873,  2595,  1010,\n",
              "         1998,  3147, 14699,  2015,  1012,  2116,  3809,  7870,  2107,  2004,\n",
              "        10958, 20536,  1010,  1041, 24290,  7865,  4295,  1010,  8387,  1006,\n",
              "         9820,  1007,  1010, 20704,  2937, 24442,  1010,  1998, 18906,  2015,\n",
              "         2024,  3303,  2011, 18191,  1012,  1996,  5816,  3754,  1997, 18191,\n",
              "         2000,  3426,  4295,  2003,  2649,  1999,  3408,  1997,  6819,  6820,\n",
              "        22717,  1012,  2060,  7870,  2024,  2104,  4812,  2000,  7523,  2065,\n",
              "         2027,  2031,  1037,  7865,  2004,  1996,  6187, 10383,  6024,  4005,\n",
              "         1010,  2107,  2004,  1996,  2825,  4434,  2090,  2529,  2014, 10374,\n",
              "        23350,  1020,  1006,  1044,  2232,  2615,  2575,  1007,  1998, 23130,\n",
              "         7870,  2107,  2004,  3674,  8040,  3917, 12650,  1998, 11888, 16342,\n",
              "         8715,  1012,  1031, 16528,  1033,  2045,  2003,  6704])"
            ]
          },
          "metadata": {},
          "execution_count": 167
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# print the first 50 tokens and corresponding labels\n",
        "for token, label in zip(tokenizer.convert_ids_to_tokens(training_set[0][\"ids\"][:50]), tokenizer.convert_ids_to_tokens(training_set[0][\"targets\"][:50])):\n",
        "  # print('{0:10} {1}'.format(token, id2label[label]))\n",
        "  print(token, label)\n"
      ],
      "metadata": {
        "id": "y6J3d3wV5bm8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e55ba3fa-4fe2-490a-f274-516bd2e84c9e"
      },
      "execution_count": 168,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CLS] [PAD]\n",
            "examples [PAD]\n",
            "of [PAD]\n",
            "common [PAD]\n",
            "human [PAD]\n",
            "diseases [PAD]\n",
            "caused [PAD]\n",
            "by [PAD]\n",
            "viruses [PAD]\n",
            "include [PAD]\n",
            "the [PAD]\n",
            "common [PAD]\n",
            "cold [PAD]\n",
            ", [PAD]\n",
            "influenza [PAD]\n",
            ", [PAD]\n",
            "chicken [PAD]\n",
            "##po [PAD]\n",
            "##x [PAD]\n",
            ", [PAD]\n",
            "and [PAD]\n",
            "cold [PAD]\n",
            "sore [PAD]\n",
            "##s [PAD]\n",
            ". [PAD]\n",
            "many [PAD]\n",
            "serious [PAD]\n",
            "diseases [PAD]\n",
            "such [PAD]\n",
            "as [PAD]\n",
            "ra [PAD]\n",
            "##bies [PAD]\n",
            ", [PAD]\n",
            "e [PAD]\n",
            "##bola [PAD]\n",
            "virus [PAD]\n",
            "disease [PAD]\n",
            ", [PAD]\n",
            "aids [PAD]\n",
            "( [PAD]\n",
            "hiv [PAD]\n",
            ") [PAD]\n",
            ", [PAD]\n",
            "av [PAD]\n",
            "##ian [PAD]\n",
            "influenza [PAD]\n",
            ", [PAD]\n",
            "and [PAD]\n",
            "sar [PAD]\n",
            "##s [PAD]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Defining the model"
      ],
      "metadata": {
        "id": "oqf7g7QYEizI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_params = {'batch_size': TRAIN_BATCH_SIZE,\n",
        "                'shuffle': True,\n",
        "                'num_workers': 0\n",
        "                }\n",
        "\n",
        "test_params = {'batch_size': VALID_BATCH_SIZE,\n",
        "                'shuffle': True,\n",
        "                'num_workers': 0\n",
        "                }\n",
        "\n",
        "training_loader = DataLoader(training_set, **train_params)\n",
        "testing_loader = DataLoader(testing_set, **test_params)"
      ],
      "metadata": {
        "id": "9XS8OInFHNQq"
      },
      "execution_count": 169,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = BertForTokenClassification.from_pretrained('bert-base-uncased', \n",
        "                                                   num_labels=len(id2label),\n",
        "                                                   id2label=id2label,\n",
        "                                                   label2id=label2id)\n",
        "model.to(device)"
      ],
      "metadata": {
        "id": "AQV0APlz5bpj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1a78777d-5706-4992-b018-2992c126ba7d"
      },
      "execution_count": 170,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForTokenClassification: ['cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias']\n",
            "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BertForTokenClassification(\n",
              "  (bert): BertModel(\n",
              "    (embeddings): BertEmbeddings(\n",
              "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
              "      (position_embeddings): Embedding(512, 768)\n",
              "      (token_type_embeddings): Embedding(2, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): BertEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (1): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (2): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (3): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (4): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (5): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (6): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (7): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (8): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (9): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (10): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (11): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (dropout): Dropout(p=0.1, inplace=False)\n",
              "  (classifier): Linear(in_features=768, out_features=4, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 170
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ids = training_set[0][\"ids\"].unsqueeze(0)\n",
        "mask = training_set[0][\"mask\"].unsqueeze(0)\n",
        "targets = training_set[0][\"targets\"].unsqueeze(0)\n",
        "ids = ids.to(device)\n",
        "mask = mask.to(device)\n",
        "targets = targets.to(device)\n",
        "outputs = model(input_ids=ids, attention_mask=mask, labels=targets)\n",
        "initial_loss = outputs[0]\n",
        "initial_loss"
      ],
      "metadata": {
        "id": "bDJKUE3N5bsX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "88253374-4964-4a1c-ce5b-edeb9c5d4ea3"
      },
      "execution_count": 171,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(1.3559, device='cuda:0', grad_fn=<NllLossBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 171
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tr_logits = outputs[1]\n",
        "tr_logits.shape"
      ],
      "metadata": {
        "id": "Lni6ZvRd5bvB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "53e5caa5-9b22-4cc2-c6d4-82e5c1f68979"
      },
      "execution_count": 172,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 128, 4])"
            ]
          },
          "metadata": {},
          "execution_count": 172
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = torch.optim.Adam(params=model.parameters(), lr=LEARNING_RATE)"
      ],
      "metadata": {
        "id": "dUZggZGz5b0w"
      },
      "execution_count": 173,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model Training"
      ],
      "metadata": {
        "id": "bqYvYxnMFCji"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Defining the training function on the 80% of the dataset for tuning the bert model\n",
        "def train(epoch):\n",
        "    tr_loss, tr_accuracy = 0, 0\n",
        "    nb_tr_examples, nb_tr_steps = 0, 0\n",
        "    tr_preds, tr_labels = [], []\n",
        "    # put model in training mode\n",
        "    model.train()\n",
        "    \n",
        "    for idx, batch in enumerate(training_loader):\n",
        "\n",
        "        ids = batch['ids'].to(device, dtype=torch.long)\n",
        "        mask = batch['mask'].to(device, dtype=torch.long)\n",
        "        targets = batch['targets'].to(device, dtype=torch.long)\n",
        "\n",
        "        outputs = model(input_ids=ids, attention_mask=mask, labels=targets)\n",
        "        loss, tr_logits = outputs.loss, outputs.logits\n",
        "        tr_loss += loss.item()\n",
        "\n",
        "        nb_tr_steps += 1\n",
        "        nb_tr_examples += targets.size(0)\n",
        "        \n",
        "        if idx % 100==0:\n",
        "            loss_step = tr_loss/nb_tr_steps\n",
        "            print(f\"Training loss per {idx} training steps: {loss_step}\")\n",
        "           \n",
        "        # compute training accuracy\n",
        "        flattened_targets = targets.view(-1) # shape (batch_size * seq_len,)\n",
        "        active_logits = tr_logits.view(-1, model.num_labels) # shape (batch_size * seq_len, num_labels)\n",
        "        flattened_predictions = torch.argmax(active_logits, axis=1) # shape (batch_size * seq_len,)\n",
        "        # now, use mask to determine where we should compare predictions with targets (includes [CLS] and [SEP] token predictions)\n",
        "        active_accuracy = mask.view(-1) == 1 # active accuracy is also of shape (batch_size * seq_len,)\n",
        "        targets = torch.masked_select(flattened_targets, active_accuracy)\n",
        "        predictions = torch.masked_select(flattened_predictions, active_accuracy)\n",
        "        \n",
        "        tr_preds.extend(predictions)\n",
        "        tr_labels.extend(targets)\n",
        "        \n",
        "        tmp_tr_accuracy = accuracy_score(targets.cpu().numpy(), predictions.cpu().numpy())\n",
        "        tr_accuracy += tmp_tr_accuracy\n",
        "    \n",
        "        # gradient clipping\n",
        "        torch.nn.utils.clip_grad_norm_(\n",
        "            parameters=model.parameters(), max_norm=MAX_GRAD_NORM\n",
        "        )\n",
        "        \n",
        "        # backward pass\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    epoch_loss = tr_loss / nb_tr_steps\n",
        "    tr_accuracy = tr_accuracy / nb_tr_steps\n",
        "    print(f\"Training loss epoch: {epoch_loss}\")\n",
        "    print(f\"Training accuracy epoch: {tr_accuracy}\")"
      ],
      "metadata": {
        "id": "QJpQijaJ5b3T"
      },
      "execution_count": 174,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "EPOCHS = 1000 # Overwriting EPOCHS mentioned in hyperparameter initialization\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    print(f\"Training epoch: {epoch + 1}\")\n",
        "    train(epoch)"
      ],
      "metadata": {
        "id": "RTRkZpKf5b5_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "12def4ab-493e-4c80-e6ce-fcb8151f2368"
      },
      "execution_count": 175,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training epoch: 1\n",
            "Training loss per 0 training steps: 1.4575726985931396\n",
            "Training loss epoch: 0.8903892834981283\n",
            "Training accuracy epoch: 0.7618631363385907\n",
            "Training epoch: 2\n",
            "Training loss per 0 training steps: 0.5635035037994385\n",
            "Training loss epoch: 0.3310677744448185\n",
            "Training accuracy epoch: 0.9445771510423354\n",
            "Training epoch: 3\n",
            "Training loss per 0 training steps: 0.1783135086297989\n",
            "Training loss epoch: 0.24367605770627657\n",
            "Training accuracy epoch: 0.9456320831720566\n",
            "Training epoch: 4\n",
            "Training loss per 0 training steps: 0.4173550605773926\n",
            "Training loss epoch: 0.212019138969481\n",
            "Training accuracy epoch: 0.9473436069802929\n",
            "Training epoch: 5\n",
            "Training loss per 0 training steps: 0.03569668158888817\n",
            "Training loss epoch: 0.19982089567929506\n",
            "Training accuracy epoch: 0.9475746967506972\n",
            "Training epoch: 6\n",
            "Training loss per 0 training steps: 0.1393236517906189\n",
            "Training loss epoch: 0.18896524732311568\n",
            "Training accuracy epoch: 0.9422693425462597\n",
            "Training epoch: 7\n",
            "Training loss per 0 training steps: 0.2545411288738251\n",
            "Training loss epoch: 0.17784535326063633\n",
            "Training accuracy epoch: 0.946602261767571\n",
            "Training epoch: 8\n",
            "Training loss per 0 training steps: 0.2030218243598938\n",
            "Training loss epoch: 0.15619639276216427\n",
            "Training accuracy epoch: 0.9471187102521612\n",
            "Training epoch: 9\n",
            "Training loss per 0 training steps: 0.22084085643291473\n",
            "Training loss epoch: 0.1399026233702898\n",
            "Training accuracy epoch: 0.9493468205410914\n",
            "Training epoch: 10\n",
            "Training loss per 0 training steps: 0.13978658616542816\n",
            "Training loss epoch: 0.12304351478815079\n",
            "Training accuracy epoch: 0.9543463138660923\n",
            "Training epoch: 11\n",
            "Training loss per 0 training steps: 0.04151380434632301\n",
            "Training loss epoch: 0.10568073950707912\n",
            "Training accuracy epoch: 0.956634183843497\n",
            "Training epoch: 12\n",
            "Training loss per 0 training steps: 0.07616803795099258\n",
            "Training loss epoch: 0.09237983915954828\n",
            "Training accuracy epoch: 0.9710272045366053\n",
            "Training epoch: 13\n",
            "Training loss per 0 training steps: 0.05555479973554611\n",
            "Training loss epoch: 0.076035506480063\n",
            "Training accuracy epoch: 0.9722003256188279\n",
            "Training epoch: 14\n",
            "Training loss per 0 training steps: 0.04733870178461075\n",
            "Training loss epoch: 0.07212146449213226\n",
            "Training accuracy epoch: 0.9747310569908372\n",
            "Training epoch: 15\n",
            "Training loss per 0 training steps: 0.08410336822271347\n",
            "Training loss epoch: 0.06407451858588804\n",
            "Training accuracy epoch: 0.9779082463657466\n",
            "Training epoch: 16\n",
            "Training loss per 0 training steps: 0.06703874468803406\n",
            "Training loss epoch: 0.05111905699595809\n",
            "Training accuracy epoch: 0.9850418672421312\n",
            "Training epoch: 17\n",
            "Training loss per 0 training steps: 0.022083623334765434\n",
            "Training loss epoch: 0.04263479635119438\n",
            "Training accuracy epoch: 0.9867058712495257\n",
            "Training epoch: 18\n",
            "Training loss per 0 training steps: 0.03236036002635956\n",
            "Training loss epoch: 0.039422273791084685\n",
            "Training accuracy epoch: 0.9858621001380117\n",
            "Training epoch: 19\n",
            "Training loss per 0 training steps: 0.05562897026538849\n",
            "Training loss epoch: 0.02997568532979737\n",
            "Training accuracy epoch: 0.9931709189364519\n",
            "Training epoch: 20\n",
            "Training loss per 0 training steps: 0.019145077094435692\n",
            "Training loss epoch: 0.029510642246653635\n",
            "Training accuracy epoch: 0.992069678535835\n",
            "Training epoch: 21\n",
            "Training loss per 0 training steps: 0.010201552882790565\n",
            "Training loss epoch: 0.02771838071445624\n",
            "Training accuracy epoch: 0.990627062875573\n",
            "Training epoch: 22\n",
            "Training loss per 0 training steps: 0.057153452187776566\n",
            "Training loss epoch: 0.029717381267497938\n",
            "Training accuracy epoch: 0.9929711646904206\n",
            "Training epoch: 23\n",
            "Training loss per 0 training steps: 0.02483415976166725\n",
            "Training loss epoch: 0.024657829373609275\n",
            "Training accuracy epoch: 0.9928449724136109\n",
            "Training epoch: 24\n",
            "Training loss per 0 training steps: 0.024137195199728012\n",
            "Training loss epoch: 0.020884464650104444\n",
            "Training accuracy epoch: 0.9957985087954228\n",
            "Training epoch: 25\n",
            "Training loss per 0 training steps: 0.012834705412387848\n",
            "Training loss epoch: 0.01954982530636092\n",
            "Training accuracy epoch: 0.994765711953212\n",
            "Training epoch: 26\n",
            "Training loss per 0 training steps: 0.02835850603878498\n",
            "Training loss epoch: 0.014434240447978178\n",
            "Training accuracy epoch: 0.9983515779426578\n",
            "Training epoch: 27\n",
            "Training loss per 0 training steps: 0.012899661436676979\n",
            "Training loss epoch: 0.0149746691264833\n",
            "Training accuracy epoch: 0.9960551493531389\n",
            "Training epoch: 28\n",
            "Training loss per 0 training steps: 0.02839033491909504\n",
            "Training loss epoch: 0.012550847042196741\n",
            "Training accuracy epoch: 0.9979629834758131\n",
            "Training epoch: 29\n",
            "Training loss per 0 training steps: 0.02464640513062477\n",
            "Training loss epoch: 0.010096894130886843\n",
            "Training accuracy epoch: 0.998207379015356\n",
            "Training epoch: 30\n",
            "Training loss per 0 training steps: 0.01307232491672039\n",
            "Training loss epoch: 0.009320707157409439\n",
            "Training accuracy epoch: 0.9983959298437147\n",
            "Training epoch: 31\n",
            "Training loss per 0 training steps: 0.0072174761444330215\n",
            "Training loss epoch: 0.010096675308886915\n",
            "Training accuracy epoch: 0.9989897629310344\n",
            "Training epoch: 32\n",
            "Training loss per 0 training steps: 0.0026099998503923416\n",
            "Training loss epoch: 0.007396899891318753\n",
            "Training accuracy epoch: 0.9996744791666666\n",
            "Training epoch: 33\n",
            "Training loss per 0 training steps: 0.009110863320529461\n",
            "Training loss epoch: 0.007178094587288797\n",
            "Training accuracy epoch: 0.9996345029239766\n",
            "Training epoch: 34\n",
            "Training loss per 0 training steps: 0.0030608936212956905\n",
            "Training loss epoch: 0.006294231512583792\n",
            "Training accuracy epoch: 0.9991418026016933\n",
            "Training epoch: 35\n",
            "Training loss per 0 training steps: 0.00693576829507947\n",
            "Training loss epoch: 0.006466930402287592\n",
            "Training accuracy epoch: 0.9994588744588745\n",
            "Training epoch: 36\n",
            "Training loss per 0 training steps: 0.004399301949888468\n",
            "Training loss epoch: 0.005492836401875441\n",
            "Training accuracy epoch: 0.9996744791666666\n",
            "Training epoch: 37\n",
            "Training loss per 0 training steps: 0.011373459361493587\n",
            "Training loss epoch: 0.005829483881825581\n",
            "Training accuracy epoch: 0.9991748977112902\n",
            "Training epoch: 38\n",
            "Training loss per 0 training steps: 0.009771974757313728\n",
            "Training loss epoch: 0.005668378308958684\n",
            "Training accuracy epoch: 0.9985981692481194\n",
            "Training epoch: 39\n",
            "Training loss per 0 training steps: 0.0008739721379242837\n",
            "Training loss epoch: 0.00646054906246718\n",
            "Training accuracy epoch: 0.99898346125731\n",
            "Training epoch: 40\n",
            "Training loss per 0 training steps: 0.001357176573947072\n",
            "Training loss epoch: 0.0050611120532266796\n",
            "Training accuracy epoch: 0.9996744791666666\n",
            "Training epoch: 41\n",
            "Training loss per 0 training steps: 0.0025837665889412165\n",
            "Training loss epoch: 0.00490260145549352\n",
            "Training accuracy epoch: 0.999648382559775\n",
            "Training epoch: 42\n",
            "Training loss per 0 training steps: 0.019643466919660568\n",
            "Training loss epoch: 0.0039679102968269335\n",
            "Training accuracy epoch: 0.9996408045977011\n",
            "Training epoch: 43\n",
            "Training loss per 0 training steps: 0.00349473487585783\n",
            "Training loss epoch: 0.0037654508681346974\n",
            "Training accuracy epoch: 0.9994588744588745\n",
            "Training epoch: 44\n",
            "Training loss per 0 training steps: 0.0009691508603282273\n",
            "Training loss epoch: 0.0040940204974807175\n",
            "Training accuracy epoch: 0.9996744791666666\n",
            "Training epoch: 45\n",
            "Training loss per 0 training steps: 0.0029790590051561594\n",
            "Training loss epoch: 0.003608535674478238\n",
            "Training accuracy epoch: 0.9995471014492754\n",
            "Training epoch: 46\n",
            "Training loss per 0 training steps: 0.0022369781509041786\n",
            "Training loss epoch: 0.0031004868991052112\n",
            "Training accuracy epoch: 0.999648382559775\n",
            "Training epoch: 47\n",
            "Training loss per 0 training steps: 0.001942341448739171\n",
            "Training loss epoch: 0.003292677715459528\n",
            "Training accuracy epoch: 0.9996345029239766\n",
            "Training epoch: 48\n",
            "Training loss per 0 training steps: 0.0021088558714836836\n",
            "Training loss epoch: 0.0030241414012076953\n",
            "Training accuracy epoch: 0.9995265151515151\n",
            "Training epoch: 49\n",
            "Training loss per 0 training steps: 0.002216694410890341\n",
            "Training loss epoch: 0.0031183783139567822\n",
            "Training accuracy epoch: 0.9991333536255412\n",
            "Training epoch: 50\n",
            "Training loss per 0 training steps: 0.0020187736954540014\n",
            "Training loss epoch: 0.0027090721850981936\n",
            "Training accuracy epoch: 0.9996345029239766\n",
            "Training epoch: 51\n",
            "Training loss per 0 training steps: 0.0021513495594263077\n",
            "Training loss epoch: 0.0024017220469734943\n",
            "Training accuracy epoch: 0.9995791245791246\n",
            "Training epoch: 52\n",
            "Training loss per 0 training steps: 0.003983024042099714\n",
            "Training loss epoch: 0.002449125652977576\n",
            "Training accuracy epoch: 0.9996744791666666\n",
            "Training epoch: 53\n",
            "Training loss per 0 training steps: 0.00618906132876873\n",
            "Training loss epoch: 0.0025470440596109256\n",
            "Training accuracy epoch: 0.9992690058479532\n",
            "Training epoch: 54\n",
            "Training loss per 0 training steps: 0.0009148032986558974\n",
            "Training loss epoch: 0.0030415380072857565\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 55\n",
            "Training loss per 0 training steps: 0.0016351311933249235\n",
            "Training loss epoch: 0.002320004772627726\n",
            "Training accuracy epoch: 0.9995265151515151\n",
            "Training epoch: 56\n",
            "Training loss per 0 training steps: 0.001572728855535388\n",
            "Training loss epoch: 0.0020658817035534107\n",
            "Training accuracy epoch: 0.9996744791666666\n",
            "Training epoch: 57\n",
            "Training loss per 0 training steps: 0.003734314814209938\n",
            "Training loss epoch: 0.001856743515721367\n",
            "Training accuracy epoch: 0.9996744791666666\n",
            "Training epoch: 58\n",
            "Training loss per 0 training steps: 0.001256386050954461\n",
            "Training loss epoch: 0.0017022793763317168\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 59\n",
            "Training loss per 0 training steps: 0.0035371780395507812\n",
            "Training loss epoch: 0.0015983865353822087\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 60\n",
            "Training loss per 0 training steps: 0.005968094803392887\n",
            "Training loss epoch: 0.0017796554311644286\n",
            "Training accuracy epoch: 0.9995791245791246\n",
            "Training epoch: 61\n",
            "Training loss per 0 training steps: 0.001091176294721663\n",
            "Training loss epoch: 0.0018282987342293684\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 62\n",
            "Training loss per 0 training steps: 0.0008644955814816058\n",
            "Training loss epoch: 0.0014878235451760702\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 63\n",
            "Training loss per 0 training steps: 0.0006999034667387605\n",
            "Training loss epoch: 0.0015968494650830205\n",
            "Training accuracy epoch: 0.9996744791666666\n",
            "Training epoch: 64\n",
            "Training loss per 0 training steps: 0.0021895552054047585\n",
            "Training loss epoch: 0.0015035027172416449\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 65\n",
            "Training loss per 0 training steps: 0.0007756513077765703\n",
            "Training loss epoch: 0.0014833258270906906\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 66\n",
            "Training loss per 0 training steps: 0.0012093635741621256\n",
            "Training loss epoch: 0.0026723580861774585\n",
            "Training accuracy epoch: 0.9995791245791246\n",
            "Training epoch: 67\n",
            "Training loss per 0 training steps: 0.0007703830488026142\n",
            "Training loss epoch: 0.001419468496654493\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 68\n",
            "Training loss per 0 training steps: 0.0011084160069003701\n",
            "Training loss epoch: 0.0013625300440859671\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 69\n",
            "Training loss per 0 training steps: 0.0012450353242456913\n",
            "Training loss epoch: 0.0012018181975387658\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 70\n",
            "Training loss per 0 training steps: 0.000961508194450289\n",
            "Training loss epoch: 0.0011087878131850932\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 71\n",
            "Training loss per 0 training steps: 0.0007339584990404546\n",
            "Training loss epoch: 0.0010648974906265114\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 72\n",
            "Training loss per 0 training steps: 0.0006599804619327188\n",
            "Training loss epoch: 0.0010258863088286792\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 73\n",
            "Training loss per 0 training steps: 0.0004508421116042882\n",
            "Training loss epoch: 0.0009544917823707996\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 74\n",
            "Training loss per 0 training steps: 0.0008003495750017464\n",
            "Training loss epoch: 0.0008124783053062856\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 75\n",
            "Training loss per 0 training steps: 0.0005911971093155444\n",
            "Training loss epoch: 0.000944915399789655\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 76\n",
            "Training loss per 0 training steps: 0.002149266190826893\n",
            "Training loss epoch: 0.0009178907469807503\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 77\n",
            "Training loss per 0 training steps: 0.001023153425194323\n",
            "Training loss epoch: 0.0009947355332163472\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 78\n",
            "Training loss per 0 training steps: 0.0006006030598655343\n",
            "Training loss epoch: 0.0008475725868872056\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 79\n",
            "Training loss per 0 training steps: 0.0008864609990268946\n",
            "Training loss epoch: 0.0007865591129908959\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 80\n",
            "Training loss per 0 training steps: 0.0012234955793246627\n",
            "Training loss epoch: 0.0008129144771373831\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 81\n",
            "Training loss per 0 training steps: 0.00036634757998399436\n",
            "Training loss epoch: 0.000777611802429116\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 82\n",
            "Training loss per 0 training steps: 0.0010674899676814675\n",
            "Training loss epoch: 0.0007293525438096063\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 83\n",
            "Training loss per 0 training steps: 0.0016641321126371622\n",
            "Training loss epoch: 0.0007517968745863376\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 84\n",
            "Training loss per 0 training steps: 0.0006238279165700078\n",
            "Training loss epoch: 0.0007096692885776671\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 85\n",
            "Training loss per 0 training steps: 0.00125323876272887\n",
            "Training loss epoch: 0.0007025012916225629\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 86\n",
            "Training loss per 0 training steps: 0.0007465329254046082\n",
            "Training loss epoch: 0.0007277199644401359\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 87\n",
            "Training loss per 0 training steps: 0.0005695833242498338\n",
            "Training loss epoch: 0.0006868867446125174\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 88\n",
            "Training loss per 0 training steps: 0.0007797374855726957\n",
            "Training loss epoch: 0.0006627606489928439\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 89\n",
            "Training loss per 0 training steps: 0.0008208288345485926\n",
            "Training loss epoch: 0.0006201990884922756\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 90\n",
            "Training loss per 0 training steps: 0.0014085124712437391\n",
            "Training loss epoch: 0.0006180819060925083\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 91\n",
            "Training loss per 0 training steps: 0.0007170704775489867\n",
            "Training loss epoch: 0.0005947464378550649\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 92\n",
            "Training loss per 0 training steps: 0.0005298120668157935\n",
            "Training loss epoch: 0.0006976890993731407\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 93\n",
            "Training loss per 0 training steps: 0.0008840097580105066\n",
            "Training loss epoch: 0.0006059432562324218\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 94\n",
            "Training loss per 0 training steps: 0.0006738767260685563\n",
            "Training loss epoch: 0.0007532407107646577\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 95\n",
            "Training loss per 0 training steps: 0.00042935970122925937\n",
            "Training loss epoch: 0.0006808966087798277\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 96\n",
            "Training loss per 0 training steps: 0.0004569292941596359\n",
            "Training loss epoch: 0.0005923560141430547\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 97\n",
            "Training loss per 0 training steps: 0.0005001584067940712\n",
            "Training loss epoch: 0.0006852214476869752\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 98\n",
            "Training loss per 0 training steps: 0.0003276601782999933\n",
            "Training loss epoch: 0.0008557752322910043\n",
            "Training accuracy epoch: 0.9996744791666666\n",
            "Training epoch: 99\n",
            "Training loss per 0 training steps: 0.0004299001884646714\n",
            "Training loss epoch: 0.0009037501877173781\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 100\n",
            "Training loss per 0 training steps: 0.00039848184678703547\n",
            "Training loss epoch: 0.0007180462610752633\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 101\n",
            "Training loss per 0 training steps: 0.0006911082891747355\n",
            "Training loss epoch: 0.0007173909859072106\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 102\n",
            "Training loss per 0 training steps: 0.0005258597084321082\n",
            "Training loss epoch: 0.001014149379140387\n",
            "Training accuracy epoch: 0.999648382559775\n",
            "Training epoch: 103\n",
            "Training loss per 0 training steps: 0.0005199956358410418\n",
            "Training loss epoch: 0.002619527801774287\n",
            "Training accuracy epoch: 0.9993489583333334\n",
            "Training epoch: 104\n",
            "Training loss per 0 training steps: 0.0007469480042345822\n",
            "Training loss epoch: 0.001076663092438442\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 105\n",
            "Training loss per 0 training steps: 0.0008803762029856443\n",
            "Training loss epoch: 0.01079625580314314\n",
            "Training accuracy epoch: 0.9966264204545454\n",
            "Training epoch: 106\n",
            "Training loss per 0 training steps: 0.002695964416489005\n",
            "Training loss epoch: 0.08445856053731404\n",
            "Training accuracy epoch: 0.9850374571144057\n",
            "Training epoch: 107\n",
            "Training loss per 0 training steps: 0.009035688824951649\n",
            "Training loss epoch: 0.018163214127222698\n",
            "Training accuracy epoch: 0.9901014307931227\n",
            "Training epoch: 108\n",
            "Training loss per 0 training steps: 0.002806645818054676\n",
            "Training loss epoch: 0.016455088237610955\n",
            "Training accuracy epoch: 0.99703033625731\n",
            "Training epoch: 109\n",
            "Training loss per 0 training steps: 0.0075605642050504684\n",
            "Training loss epoch: 0.007278307826103021\n",
            "Training accuracy epoch: 0.9986979166666666\n",
            "Training epoch: 110\n",
            "Training loss per 0 training steps: 0.0038609805051237345\n",
            "Training loss epoch: 0.002838803338818252\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 111\n",
            "Training loss per 0 training steps: 0.0017419530777260661\n",
            "Training loss epoch: 0.0017169769901859884\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 112\n",
            "Training loss per 0 training steps: 0.0015891549410298467\n",
            "Training loss epoch: 0.0013153827846205484\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 113\n",
            "Training loss per 0 training steps: 0.0013720530550926924\n",
            "Training loss epoch: 0.0011461292087915353\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 114\n",
            "Training loss per 0 training steps: 0.0014020608505234122\n",
            "Training loss epoch: 0.0010829343179163213\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 115\n",
            "Training loss per 0 training steps: 0.0013119785580784082\n",
            "Training loss epoch: 0.0009591616956943957\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 116\n",
            "Training loss per 0 training steps: 0.0007507561822421849\n",
            "Training loss epoch: 0.001263011993917947\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 117\n",
            "Training loss per 0 training steps: 0.0007176476065069437\n",
            "Training loss epoch: 0.0007867048673991425\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 118\n",
            "Training loss per 0 training steps: 0.0012306225253269076\n",
            "Training loss epoch: 0.0008223879437233942\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 119\n",
            "Training loss per 0 training steps: 0.0005297755124047399\n",
            "Training loss epoch: 0.0007756942407771324\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 120\n",
            "Training loss per 0 training steps: 0.0011222227476537228\n",
            "Training loss epoch: 0.0017276170813905385\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 121\n",
            "Training loss per 0 training steps: 0.0008389552240259945\n",
            "Training loss epoch: 0.0006494356081626999\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 122\n",
            "Training loss per 0 training steps: 0.00043623693636618555\n",
            "Training loss epoch: 0.0006410116329789162\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 123\n",
            "Training loss per 0 training steps: 0.000518828455824405\n",
            "Training loss epoch: 0.0006132737277463699\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 124\n",
            "Training loss per 0 training steps: 0.0008370744180865586\n",
            "Training loss epoch: 0.0006260766798125891\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 125\n",
            "Training loss per 0 training steps: 0.00024528379435651004\n",
            "Training loss epoch: 0.0009523039504225986\n",
            "Training accuracy epoch: 0.9996744791666666\n",
            "Training epoch: 126\n",
            "Training loss per 0 training steps: 0.0010053011355921626\n",
            "Training loss epoch: 0.0006253737325702483\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 127\n",
            "Training loss per 0 training steps: 0.000564105692319572\n",
            "Training loss epoch: 0.0005661924151354469\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 128\n",
            "Training loss per 0 training steps: 0.0006741931429132819\n",
            "Training loss epoch: 0.0014300863064515095\n",
            "Training accuracy epoch: 0.9995726495726496\n",
            "Training epoch: 129\n",
            "Training loss per 0 training steps: 0.0005428571603260934\n",
            "Training loss epoch: 0.0005511410393713353\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 130\n",
            "Training loss per 0 training steps: 0.0002981614088639617\n",
            "Training loss epoch: 0.0005095424906661113\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 131\n",
            "Training loss per 0 training steps: 0.0008174492395482957\n",
            "Training loss epoch: 0.0005491987855445283\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 132\n",
            "Training loss per 0 training steps: 0.0007397975423373282\n",
            "Training loss epoch: 0.0005325817777096139\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 133\n",
            "Training loss per 0 training steps: 0.0005805135588161647\n",
            "Training loss epoch: 0.00046715137189797434\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 134\n",
            "Training loss per 0 training steps: 0.0005477844388224185\n",
            "Training loss epoch: 0.0004688527842517942\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 135\n",
            "Training loss per 0 training steps: 0.00037837540730834007\n",
            "Training loss epoch: 0.0004283011391332063\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 136\n",
            "Training loss per 0 training steps: 0.0003147647075820714\n",
            "Training loss epoch: 0.00046236350560017553\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 137\n",
            "Training loss per 0 training steps: 0.00034985289676114917\n",
            "Training loss epoch: 0.0004029358472810903\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 138\n",
            "Training loss per 0 training steps: 0.000494890846312046\n",
            "Training loss epoch: 0.00040700751803039264\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 139\n",
            "Training loss per 0 training steps: 0.0003569263208191842\n",
            "Training loss epoch: 0.0004318069841247052\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 140\n",
            "Training loss per 0 training steps: 0.0004005474329460412\n",
            "Training loss epoch: 0.0004056344902589141\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 141\n",
            "Training loss per 0 training steps: 0.0003162749344483018\n",
            "Training loss epoch: 0.0003992482209772182\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 142\n",
            "Training loss per 0 training steps: 0.0005539797712117434\n",
            "Training loss epoch: 0.00041498811818504083\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 143\n",
            "Training loss per 0 training steps: 0.000217104097828269\n",
            "Training loss epoch: 0.00038141578988870606\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 144\n",
            "Training loss per 0 training steps: 0.00041517498902976513\n",
            "Training loss epoch: 0.0003583888731858072\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 145\n",
            "Training loss per 0 training steps: 0.000552173878531903\n",
            "Training loss epoch: 0.0004170715898605219\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 146\n",
            "Training loss per 0 training steps: 0.00034150012652389705\n",
            "Training loss epoch: 0.00038164668027699616\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 147\n",
            "Training loss per 0 training steps: 0.0004096159536857158\n",
            "Training loss epoch: 0.0003487056953114613\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 148\n",
            "Training loss per 0 training steps: 0.00036756167537532747\n",
            "Training loss epoch: 0.00035060846494161524\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 149\n",
            "Training loss per 0 training steps: 0.0002349515852984041\n",
            "Training loss epoch: 0.00037725636987791705\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 150\n",
            "Training loss per 0 training steps: 0.00033128593349829316\n",
            "Training loss epoch: 0.00033278083719778806\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 151\n",
            "Training loss per 0 training steps: 0.0005675953580066562\n",
            "Training loss epoch: 0.00034119909832952544\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 152\n",
            "Training loss per 0 training steps: 0.0003009081119671464\n",
            "Training loss epoch: 0.000462062324610694\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 153\n",
            "Training loss per 0 training steps: 0.00019756625988520682\n",
            "Training loss epoch: 0.0003308570012450218\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 154\n",
            "Training loss per 0 training steps: 0.00025152089074254036\n",
            "Training loss epoch: 0.00032938376534730196\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 155\n",
            "Training loss per 0 training steps: 0.0002751914144027978\n",
            "Training loss epoch: 0.00032481448821878683\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 156\n",
            "Training loss per 0 training steps: 0.0003589956904761493\n",
            "Training loss epoch: 0.0005394202986887345\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 157\n",
            "Training loss per 0 training steps: 0.0004363141779322177\n",
            "Training loss epoch: 0.0002980347211026431\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 158\n",
            "Training loss per 0 training steps: 0.0003907959326170385\n",
            "Training loss epoch: 0.0003346346808636251\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 159\n",
            "Training loss per 0 training steps: 0.00023940594110172242\n",
            "Training loss epoch: 0.00028661478791036643\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 160\n",
            "Training loss per 0 training steps: 0.00027853131177835166\n",
            "Training loss epoch: 0.0002838648845984911\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 161\n",
            "Training loss per 0 training steps: 0.00038616673555225134\n",
            "Training loss epoch: 0.000305215372160698\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 162\n",
            "Training loss per 0 training steps: 0.00023338492610491812\n",
            "Training loss epoch: 0.00031062569056909223\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 163\n",
            "Training loss per 0 training steps: 0.000307920592604205\n",
            "Training loss epoch: 0.0002784610975747152\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 164\n",
            "Training loss per 0 training steps: 0.00018228233966510743\n",
            "Training loss epoch: 0.0002917057414985417\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 165\n",
            "Training loss per 0 training steps: 0.00020495128410402685\n",
            "Training loss epoch: 0.0003324469435028732\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 166\n",
            "Training loss per 0 training steps: 0.00025219263625331223\n",
            "Training loss epoch: 0.0002775007742457092\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 167\n",
            "Training loss per 0 training steps: 0.00027719439822249115\n",
            "Training loss epoch: 0.00026241377781843767\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 168\n",
            "Training loss per 0 training steps: 0.0003018787829205394\n",
            "Training loss epoch: 0.00027343524076665443\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 169\n",
            "Training loss per 0 training steps: 0.00024399033281952143\n",
            "Training loss epoch: 0.0002899316083736873\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 170\n",
            "Training loss per 0 training steps: 0.0001559690572321415\n",
            "Training loss epoch: 0.00026205088579445146\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 171\n",
            "Training loss per 0 training steps: 0.00026006766711361706\n",
            "Training loss epoch: 0.00026313022317481227\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 172\n",
            "Training loss per 0 training steps: 0.00021046545589342713\n",
            "Training loss epoch: 0.0002540535951993661\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 173\n",
            "Training loss per 0 training steps: 0.00030051666544750333\n",
            "Training loss epoch: 0.00025182462923112325\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 174\n",
            "Training loss per 0 training steps: 0.0001284284080611542\n",
            "Training loss epoch: 0.000251333462074399\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 175\n",
            "Training loss per 0 training steps: 0.0002451287000440061\n",
            "Training loss epoch: 0.00025005179243938375\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 176\n",
            "Training loss per 0 training steps: 0.00020470554591156542\n",
            "Training loss epoch: 0.00024913419838412665\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 177\n",
            "Training loss per 0 training steps: 0.0003237580240238458\n",
            "Training loss epoch: 0.00026557188903097995\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 178\n",
            "Training loss per 0 training steps: 0.00030360324308276176\n",
            "Training loss epoch: 0.00025197204983366345\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 179\n",
            "Training loss per 0 training steps: 0.00019627594156190753\n",
            "Training loss epoch: 0.00023550071758412136\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 180\n",
            "Training loss per 0 training steps: 0.00024909147759899497\n",
            "Training loss epoch: 0.0002290029042342212\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 181\n",
            "Training loss per 0 training steps: 0.00029348986572586\n",
            "Training loss epoch: 0.00021970402303850278\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 182\n",
            "Training loss per 0 training steps: 0.0003352555795572698\n",
            "Training loss epoch: 0.00023853479918519346\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 183\n",
            "Training loss per 0 training steps: 0.00024336019123438746\n",
            "Training loss epoch: 0.0002333486560625412\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 184\n",
            "Training loss per 0 training steps: 0.0002620041777845472\n",
            "Training loss epoch: 0.0002528486644829779\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 185\n",
            "Training loss per 0 training steps: 0.0001332778192590922\n",
            "Training loss epoch: 0.00023410693393088877\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 186\n",
            "Training loss per 0 training steps: 0.0003633459855336696\n",
            "Training loss epoch: 0.00021822408477116065\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 187\n",
            "Training loss per 0 training steps: 0.00030168998637236655\n",
            "Training loss epoch: 0.00022464770033063056\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 188\n",
            "Training loss per 0 training steps: 0.00013196752115618438\n",
            "Training loss epoch: 0.00022696779099836326\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 189\n",
            "Training loss per 0 training steps: 0.0001786438951967284\n",
            "Training loss epoch: 0.00021530700663182264\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 190\n",
            "Training loss per 0 training steps: 0.00026222746237181127\n",
            "Training loss epoch: 0.00021336820524690361\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 191\n",
            "Training loss per 0 training steps: 0.0002205504715675488\n",
            "Training loss epoch: 0.0002190663738777706\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 192\n",
            "Training loss per 0 training steps: 0.00023920074454508722\n",
            "Training loss epoch: 0.00019816964595520403\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 193\n",
            "Training loss per 0 training steps: 0.00021341253886930645\n",
            "Training loss epoch: 0.0002059627149719745\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 194\n",
            "Training loss per 0 training steps: 0.00018480337166693062\n",
            "Training loss epoch: 0.0002103101542161312\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 195\n",
            "Training loss per 0 training steps: 0.00015168898971751332\n",
            "Training loss epoch: 0.0001987115028896369\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 196\n",
            "Training loss per 0 training steps: 0.00015573915152344853\n",
            "Training loss epoch: 0.0002020917866805879\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 197\n",
            "Training loss per 0 training steps: 0.0001462592917960137\n",
            "Training loss epoch: 0.00020482830526210213\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 198\n",
            "Training loss per 0 training steps: 0.00012114385754102841\n",
            "Training loss epoch: 0.0002002927412831923\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 199\n",
            "Training loss per 0 training steps: 0.00011047048610635102\n",
            "Training loss epoch: 0.00018739518175910538\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 200\n",
            "Training loss per 0 training steps: 0.00023265350318979472\n",
            "Training loss epoch: 0.00019684343108868538\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 201\n",
            "Training loss per 0 training steps: 0.00017605275206733495\n",
            "Training loss epoch: 0.00019221739057684317\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 202\n",
            "Training loss per 0 training steps: 0.00023675139527767897\n",
            "Training loss epoch: 0.00023363359287031926\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 203\n",
            "Training loss per 0 training steps: 0.00017139638657681644\n",
            "Training loss epoch: 0.00020283766207285225\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 204\n",
            "Training loss per 0 training steps: 0.00017353739531245083\n",
            "Training loss epoch: 0.00018900566525796117\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 205\n",
            "Training loss per 0 training steps: 0.00021533344988711178\n",
            "Training loss epoch: 0.00018692732555791736\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 206\n",
            "Training loss per 0 training steps: 0.00016056395543273538\n",
            "Training loss epoch: 0.00018211125279776752\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 207\n",
            "Training loss per 0 training steps: 0.00019015349971596152\n",
            "Training loss epoch: 0.00017946465110677914\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 208\n",
            "Training loss per 0 training steps: 0.00018430450290907174\n",
            "Training loss epoch: 0.0001893170592666138\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 209\n",
            "Training loss per 0 training steps: 0.00017388885316904634\n",
            "Training loss epoch: 0.00026609549604472704\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 210\n",
            "Training loss per 0 training steps: 0.00013816099090036005\n",
            "Training loss epoch: 0.00023647315174457617\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 211\n",
            "Training loss per 0 training steps: 0.00024292290618177503\n",
            "Training loss epoch: 0.001144187807464429\n",
            "Training accuracy epoch: 0.9996744791666666\n",
            "Training epoch: 212\n",
            "Training loss per 0 training steps: 0.0003516429278533906\n",
            "Training loss epoch: 0.0024256830547528807\n",
            "Training accuracy epoch: 0.9993489583333334\n",
            "Training epoch: 213\n",
            "Training loss per 0 training steps: 0.0006937698344700038\n",
            "Training loss epoch: 0.012080906935201105\n",
            "Training accuracy epoch: 0.9953732248654124\n",
            "Training epoch: 214\n",
            "Training loss per 0 training steps: 0.028789762407541275\n",
            "Training loss epoch: 0.016492984805760596\n",
            "Training accuracy epoch: 0.9952558168436879\n",
            "Training epoch: 215\n",
            "Training loss per 0 training steps: 0.0005699287867173553\n",
            "Training loss epoch: 0.003980556090634006\n",
            "Training accuracy epoch: 0.9990942028985508\n",
            "Training epoch: 216\n",
            "Training loss per 0 training steps: 0.00039749257848598063\n",
            "Training loss epoch: 0.0008423783341034626\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 217\n",
            "Training loss per 0 training steps: 0.0019556055776774883\n",
            "Training loss epoch: 0.0008387431007577106\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 218\n",
            "Training loss per 0 training steps: 0.00041387160308659077\n",
            "Training loss epoch: 0.0006511432632881528\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 219\n",
            "Training loss per 0 training steps: 0.0008024976123124361\n",
            "Training loss epoch: 0.0006265756189047048\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 220\n",
            "Training loss per 0 training steps: 0.0003312855842523277\n",
            "Training loss epoch: 0.00046675186725527357\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 221\n",
            "Training loss per 0 training steps: 0.0004618952807504684\n",
            "Training loss epoch: 0.0003980719217603716\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 222\n",
            "Training loss per 0 training steps: 0.000326160981785506\n",
            "Training loss epoch: 0.0003663237912405748\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 223\n",
            "Training loss per 0 training steps: 0.0005448331357911229\n",
            "Training loss epoch: 0.00039249052497325465\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 224\n",
            "Training loss per 0 training steps: 0.00033145619090646505\n",
            "Training loss epoch: 0.00032187999386223964\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 225\n",
            "Training loss per 0 training steps: 0.0002040654799202457\n",
            "Training loss epoch: 0.00034580522575803724\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 226\n",
            "Training loss per 0 training steps: 0.0002383211103733629\n",
            "Training loss epoch: 0.0002949133789419041\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 227\n",
            "Training loss per 0 training steps: 0.00022308126790449023\n",
            "Training loss epoch: 0.00029486060035803047\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 228\n",
            "Training loss per 0 training steps: 0.00024338846560567617\n",
            "Training loss epoch: 0.00030973023967817426\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 229\n",
            "Training loss per 0 training steps: 0.00021748762810602784\n",
            "Training loss epoch: 0.000278620626583385\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 230\n",
            "Training loss per 0 training steps: 0.00035151588963344693\n",
            "Training loss epoch: 0.0002851755828790677\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 231\n",
            "Training loss per 0 training steps: 0.00046006322372704744\n",
            "Training loss epoch: 0.0002691232512006536\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 232\n",
            "Training loss per 0 training steps: 0.00024906263570301235\n",
            "Training loss epoch: 0.00025180708689731546\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 233\n",
            "Training loss per 0 training steps: 0.0002225043426733464\n",
            "Training loss epoch: 0.00024651610510773025\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 234\n",
            "Training loss per 0 training steps: 0.00016792764654383063\n",
            "Training loss epoch: 0.00023665591773654646\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 235\n",
            "Training loss per 0 training steps: 0.00024108629440888762\n",
            "Training loss epoch: 0.0002310045371511175\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 236\n",
            "Training loss per 0 training steps: 0.0002219236339442432\n",
            "Training loss epoch: 0.00023044641420710832\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 237\n",
            "Training loss per 0 training steps: 0.0002466223668307066\n",
            "Training loss epoch: 0.00021720283863639148\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 238\n",
            "Training loss per 0 training steps: 0.00020002524252049625\n",
            "Training loss epoch: 0.000227673132030759\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 239\n",
            "Training loss per 0 training steps: 0.00018949844525195658\n",
            "Training loss epoch: 0.00021295784123746367\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 240\n",
            "Training loss per 0 training steps: 0.0002024145214818418\n",
            "Training loss epoch: 0.000213839009423585\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 241\n",
            "Training loss per 0 training steps: 0.00015430858184117824\n",
            "Training loss epoch: 0.0002164383349736454\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 242\n",
            "Training loss per 0 training steps: 0.00023475704074371606\n",
            "Training loss epoch: 0.00021692446898669004\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 243\n",
            "Training loss per 0 training steps: 0.0001965156989172101\n",
            "Training loss epoch: 0.00023400088624233226\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 244\n",
            "Training loss per 0 training steps: 0.0002127884654328227\n",
            "Training loss epoch: 0.00019799544679699466\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 245\n",
            "Training loss per 0 training steps: 0.00028653143090195954\n",
            "Training loss epoch: 0.00018939250488377488\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 246\n",
            "Training loss per 0 training steps: 0.00021064412430860102\n",
            "Training loss epoch: 0.0001922262787653987\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 247\n",
            "Training loss per 0 training steps: 0.0001782401232048869\n",
            "Training loss epoch: 0.0001899119803662567\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 248\n",
            "Training loss per 0 training steps: 0.0002280046173837036\n",
            "Training loss epoch: 0.0001780096863512881\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 249\n",
            "Training loss per 0 training steps: 0.00011635410191956908\n",
            "Training loss epoch: 0.0001845807334272346\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 250\n",
            "Training loss per 0 training steps: 0.00017152515647467226\n",
            "Training loss epoch: 0.00017254898799971366\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 251\n",
            "Training loss per 0 training steps: 0.00018699473002925515\n",
            "Training loss epoch: 0.00018190734166031083\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 252\n",
            "Training loss per 0 training steps: 0.0001217861645272933\n",
            "Training loss epoch: 0.00017991602787030084\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 253\n",
            "Training loss per 0 training steps: 0.00013615695934277028\n",
            "Training loss epoch: 0.00017951569437476186\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 254\n",
            "Training loss per 0 training steps: 0.00018280823132954538\n",
            "Training loss epoch: 0.00016620477193403835\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 255\n",
            "Training loss per 0 training steps: 0.0001443018700229004\n",
            "Training loss epoch: 0.0001664362019558515\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 256\n",
            "Training loss per 0 training steps: 0.00014126217865850776\n",
            "Training loss epoch: 0.00017269153431698214\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 257\n",
            "Training loss per 0 training steps: 0.0002361734223086387\n",
            "Training loss epoch: 0.0001648104498599423\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 258\n",
            "Training loss per 0 training steps: 0.0002305375674040988\n",
            "Training loss epoch: 0.00016180405327759217\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 259\n",
            "Training loss per 0 training steps: 0.00017101930279750377\n",
            "Training loss epoch: 0.0001670927983165408\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 260\n",
            "Training loss per 0 training steps: 0.0001541115780128166\n",
            "Training loss epoch: 0.00015776607445635213\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 261\n",
            "Training loss per 0 training steps: 0.00016950069402810186\n",
            "Training loss epoch: 0.00019838517255266197\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 262\n",
            "Training loss per 0 training steps: 0.00018181056657340378\n",
            "Training loss epoch: 0.0001574024142125078\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 263\n",
            "Training loss per 0 training steps: 0.00022660546528641135\n",
            "Training loss epoch: 0.00016135533466391885\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 264\n",
            "Training loss per 0 training steps: 0.00013506026880349964\n",
            "Training loss epoch: 0.00015518506734224502\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 265\n",
            "Training loss per 0 training steps: 0.0001496569748269394\n",
            "Training loss epoch: 0.0001598363360244548\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 266\n",
            "Training loss per 0 training steps: 0.0001118380605475977\n",
            "Training loss epoch: 0.00015664765608865613\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 267\n",
            "Training loss per 0 training steps: 0.00017144295270554721\n",
            "Training loss epoch: 0.00015088376858329866\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 268\n",
            "Training loss per 0 training steps: 0.000146464939462021\n",
            "Training loss epoch: 0.0001499019296412977\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 269\n",
            "Training loss per 0 training steps: 0.00012334984785411507\n",
            "Training loss epoch: 0.00014188159608844822\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 270\n",
            "Training loss per 0 training steps: 0.0001224484876729548\n",
            "Training loss epoch: 0.00015641463566377448\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 271\n",
            "Training loss per 0 training steps: 0.00013727984332945198\n",
            "Training loss epoch: 0.0001354738227140236\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 272\n",
            "Training loss per 0 training steps: 0.00011218610598007217\n",
            "Training loss epoch: 0.00013655184920935426\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 273\n",
            "Training loss per 0 training steps: 0.00012544500350486487\n",
            "Training loss epoch: 0.00013854534396765908\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 274\n",
            "Training loss per 0 training steps: 0.00011280615581199527\n",
            "Training loss epoch: 0.00014270413278912505\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 275\n",
            "Training loss per 0 training steps: 0.00010288402700098231\n",
            "Training loss epoch: 0.00014789822004483236\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 276\n",
            "Training loss per 0 training steps: 0.00019018782768398523\n",
            "Training loss epoch: 0.00014177864068187773\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 277\n",
            "Training loss per 0 training steps: 0.00011700221512001008\n",
            "Training loss epoch: 0.00015800300207047258\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 278\n",
            "Training loss per 0 training steps: 9.529425005894154e-05\n",
            "Training loss epoch: 0.00012908583509367114\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 279\n",
            "Training loss per 0 training steps: 0.00014505756553262472\n",
            "Training loss epoch: 0.00013747172124567442\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 280\n",
            "Training loss per 0 training steps: 0.00014075757644604892\n",
            "Training loss epoch: 0.0001326769391501633\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 281\n",
            "Training loss per 0 training steps: 0.00014206765627022833\n",
            "Training loss epoch: 0.00012682853897179788\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 282\n",
            "Training loss per 0 training steps: 0.00015154069114942104\n",
            "Training loss epoch: 0.00018743543114396743\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 283\n",
            "Training loss per 0 training steps: 8.405990229221061e-05\n",
            "Training loss epoch: 0.00013111840174436415\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 284\n",
            "Training loss per 0 training steps: 0.0001251140347449109\n",
            "Training loss epoch: 0.00012896188612406453\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 285\n",
            "Training loss per 0 training steps: 0.00013403540651779622\n",
            "Training loss epoch: 0.0001271392096289977\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 286\n",
            "Training loss per 0 training steps: 0.00016449969552922994\n",
            "Training loss epoch: 0.00013799705205504628\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 287\n",
            "Training loss per 0 training steps: 7.728433411102742e-05\n",
            "Training loss epoch: 0.00013060623859928455\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 288\n",
            "Training loss per 0 training steps: 0.00017519702669233084\n",
            "Training loss epoch: 0.00013374532560798494\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 289\n",
            "Training loss per 0 training steps: 0.00014830936561338603\n",
            "Training loss epoch: 0.00012475674460195782\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 290\n",
            "Training loss per 0 training steps: 0.00011382729280740023\n",
            "Training loss epoch: 0.0001249100714630913\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 291\n",
            "Training loss per 0 training steps: 0.00014922962873242795\n",
            "Training loss epoch: 0.002540177210297164\n",
            "Training accuracy epoch: 0.9993489583333334\n",
            "Training epoch: 292\n",
            "Training loss per 0 training steps: 0.0001439835614291951\n",
            "Training loss epoch: 0.02644233738586384\n",
            "Training accuracy epoch: 0.99609375\n",
            "Training epoch: 293\n",
            "Training loss per 0 training steps: 0.00011749027180485427\n",
            "Training loss epoch: 0.0024474125323952953\n",
            "Training accuracy epoch: 0.9993489583333334\n",
            "Training epoch: 294\n",
            "Training loss per 0 training steps: 0.0006116370786912739\n",
            "Training loss epoch: 0.0018217576277190044\n",
            "Training accuracy epoch: 0.9996744791666666\n",
            "Training epoch: 295\n",
            "Training loss per 0 training steps: 0.00028498267056420445\n",
            "Training loss epoch: 0.0008356056447761754\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 296\n",
            "Training loss per 0 training steps: 0.0003085008356720209\n",
            "Training loss epoch: 0.0005635338372182256\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 297\n",
            "Training loss per 0 training steps: 0.0015133393462747335\n",
            "Training loss epoch: 0.0004089802653955606\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 298\n",
            "Training loss per 0 training steps: 0.0002852269390132278\n",
            "Training loss epoch: 0.0004311644467331159\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 299\n",
            "Training loss per 0 training steps: 0.00022144734975881875\n",
            "Training loss epoch: 0.000267701852862956\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 300\n",
            "Training loss per 0 training steps: 0.00018147722585126758\n",
            "Training loss epoch: 0.00024016178819389702\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 301\n",
            "Training loss per 0 training steps: 0.00020511944603640586\n",
            "Training loss epoch: 0.00024326676672596173\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 302\n",
            "Training loss per 0 training steps: 0.000210354890441522\n",
            "Training loss epoch: 0.0002093873275346899\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 303\n",
            "Training loss per 0 training steps: 0.00030714189051650465\n",
            "Training loss epoch: 0.00020104185629558438\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 304\n",
            "Training loss per 0 training steps: 0.0002720980264712125\n",
            "Training loss epoch: 0.0001987657239321076\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 305\n",
            "Training loss per 0 training steps: 0.00019178200454916805\n",
            "Training loss epoch: 0.00019526727434519367\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 306\n",
            "Training loss per 0 training steps: 0.00017160795687232167\n",
            "Training loss epoch: 0.0001820966287292928\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 307\n",
            "Training loss per 0 training steps: 0.00021074579854030162\n",
            "Training loss epoch: 0.00017813985929630385\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 308\n",
            "Training loss per 0 training steps: 0.00018874771194532514\n",
            "Training loss epoch: 0.00017811159341363236\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 309\n",
            "Training loss per 0 training steps: 0.0001548761792946607\n",
            "Training loss epoch: 0.0001693894919299055\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 310\n",
            "Training loss per 0 training steps: 0.000211368766031228\n",
            "Training loss epoch: 0.00017914798627316486\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 311\n",
            "Training loss per 0 training steps: 0.00016360798326786608\n",
            "Training loss epoch: 0.0001701429343180886\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 312\n",
            "Training loss per 0 training steps: 0.00013560938532464206\n",
            "Training loss epoch: 0.00016854266323207412\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 313\n",
            "Training loss per 0 training steps: 0.00019855162827298045\n",
            "Training loss epoch: 0.00015341391493469322\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 314\n",
            "Training loss per 0 training steps: 0.00012135185534134507\n",
            "Training loss epoch: 0.0001564044966168391\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 315\n",
            "Training loss per 0 training steps: 0.00013083283556625247\n",
            "Training loss epoch: 0.000140684878109217\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 316\n",
            "Training loss per 0 training steps: 0.00016234921349678189\n",
            "Training loss epoch: 0.00014798607420137463\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 317\n",
            "Training loss per 0 training steps: 0.00020938710076734424\n",
            "Training loss epoch: 0.00014986484287267862\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 318\n",
            "Training loss per 0 training steps: 8.427175635006279e-05\n",
            "Training loss epoch: 0.00014562534245972833\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 319\n",
            "Training loss per 0 training steps: 0.00014766443928238004\n",
            "Training loss epoch: 0.00013644661279007172\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 320\n",
            "Training loss per 0 training steps: 0.00010165003186557442\n",
            "Training loss epoch: 0.0001415267482419343\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 321\n",
            "Training loss per 0 training steps: 0.00012405500456225127\n",
            "Training loss epoch: 0.0007553210883391633\n",
            "Training accuracy epoch: 0.9995567375886525\n",
            "Training epoch: 322\n",
            "Training loss per 0 training steps: 0.0001606691803317517\n",
            "Training loss epoch: 0.0003228717268939363\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 323\n",
            "Training loss per 0 training steps: 9.54777278820984e-05\n",
            "Training loss epoch: 0.00014552814354829025\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 324\n",
            "Training loss per 0 training steps: 0.00012058327411068603\n",
            "Training loss epoch: 0.00018076062891244268\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 325\n",
            "Training loss per 0 training steps: 0.00010440256301080808\n",
            "Training loss epoch: 0.00014469526346753506\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 326\n",
            "Training loss per 0 training steps: 0.00015700342191848904\n",
            "Training loss epoch: 0.00013305361668850915\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 327\n",
            "Training loss per 0 training steps: 0.0001297931739827618\n",
            "Training loss epoch: 0.00014142977731050146\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 328\n",
            "Training loss per 0 training steps: 0.00010302743612555787\n",
            "Training loss epoch: 0.00013431100584663605\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 329\n",
            "Training loss per 0 training steps: 9.010653593577445e-05\n",
            "Training loss epoch: 0.00016298772546482118\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 330\n",
            "Training loss per 0 training steps: 0.0001286001061089337\n",
            "Training loss epoch: 0.0001501805236330256\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 331\n",
            "Training loss per 0 training steps: 0.0001778430596459657\n",
            "Training loss epoch: 0.00012272695797340324\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 332\n",
            "Training loss per 0 training steps: 9.413500083610415e-05\n",
            "Training loss epoch: 0.0001234283445228357\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 333\n",
            "Training loss per 0 training steps: 0.00011666741920635104\n",
            "Training loss epoch: 0.00012390673449165965\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 334\n",
            "Training loss per 0 training steps: 0.00014532906061504036\n",
            "Training loss epoch: 0.00011655236691391717\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 335\n",
            "Training loss per 0 training steps: 0.00010167990694753826\n",
            "Training loss epoch: 0.0001233070803815887\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 336\n",
            "Training loss per 0 training steps: 9.974632848752663e-05\n",
            "Training loss epoch: 0.00011452947001089342\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 337\n",
            "Training loss per 0 training steps: 0.00013066132669337094\n",
            "Training loss epoch: 0.00011736476941829703\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 338\n",
            "Training loss per 0 training steps: 0.00014829797146376222\n",
            "Training loss epoch: 0.00011910583816643339\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 339\n",
            "Training loss per 0 training steps: 0.00010316893894923851\n",
            "Training loss epoch: 0.00011295685969040885\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 340\n",
            "Training loss per 0 training steps: 0.0001209614347317256\n",
            "Training loss epoch: 0.0001131464899420583\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 341\n",
            "Training loss per 0 training steps: 0.0001036827961797826\n",
            "Training loss epoch: 0.00011371289171317282\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 342\n",
            "Training loss per 0 training steps: 8.655530109535903e-05\n",
            "Training loss epoch: 0.0001109967524826061\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 343\n",
            "Training loss per 0 training steps: 8.751526183914393e-05\n",
            "Training loss epoch: 0.0001122776357078692\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 344\n",
            "Training loss per 0 training steps: 9.507523645879701e-05\n",
            "Training loss epoch: 0.00010939276823288917\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 345\n",
            "Training loss per 0 training steps: 0.00011379018542356789\n",
            "Training loss epoch: 0.00010526553948390453\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 346\n",
            "Training loss per 0 training steps: 7.262326107593253e-05\n",
            "Training loss epoch: 0.00010796410121353499\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 347\n",
            "Training loss per 0 training steps: 9.864608728094026e-05\n",
            "Training loss epoch: 0.00010113149983226322\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 348\n",
            "Training loss per 0 training steps: 8.278431050712243e-05\n",
            "Training loss epoch: 0.00010245801786368247\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 349\n",
            "Training loss per 0 training steps: 0.00012204340600874275\n",
            "Training loss epoch: 0.00010685803924085728\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 350\n",
            "Training loss per 0 training steps: 8.25484239612706e-05\n",
            "Training loss epoch: 0.00010085038108324322\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 351\n",
            "Training loss per 0 training steps: 8.659343438921496e-05\n",
            "Training loss epoch: 9.987819915598568e-05\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 352\n",
            "Training loss per 0 training steps: 0.00011923964484594762\n",
            "Training loss epoch: 0.0001032718397861269\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 353\n",
            "Training loss per 0 training steps: 0.00011800954962382093\n",
            "Training loss epoch: 9.97173847281374e-05\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 354\n",
            "Training loss per 0 training steps: 5.602290912065655e-05\n",
            "Training loss epoch: 0.00010182894402532838\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 355\n",
            "Training loss per 0 training steps: 9.96403832687065e-05\n",
            "Training loss epoch: 0.00010070243600542501\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 356\n",
            "Training loss per 0 training steps: 8.974383672466502e-05\n",
            "Training loss epoch: 0.00010258195652568247\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 357\n",
            "Training loss per 0 training steps: 8.59346182551235e-05\n",
            "Training loss epoch: 9.415490118650875e-05\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 358\n",
            "Training loss per 0 training steps: 0.00012101230095140636\n",
            "Training loss epoch: 9.27582123040338e-05\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 359\n",
            "Training loss per 0 training steps: 8.544042066205293e-05\n",
            "Training loss epoch: 0.0001036102573076884\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 360\n",
            "Training loss per 0 training steps: 7.051792636048049e-05\n",
            "Training loss epoch: 9.512144242762588e-05\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 361\n",
            "Training loss per 0 training steps: 0.0001286548940697685\n",
            "Training loss epoch: 9.268346881678251e-05\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 362\n",
            "Training loss per 0 training steps: 5.737928586313501e-05\n",
            "Training loss epoch: 9.110507077518075e-05\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 363\n",
            "Training loss per 0 training steps: 7.93473664089106e-05\n",
            "Training loss epoch: 9.52918738524507e-05\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 364\n",
            "Training loss per 0 training steps: 9.308174048783258e-05\n",
            "Training loss epoch: 9.146132075935991e-05\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 365\n",
            "Training loss per 0 training steps: 7.231310883071274e-05\n",
            "Training loss epoch: 9.081488557664368e-05\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 366\n",
            "Training loss per 0 training steps: 8.86206908035092e-05\n",
            "Training loss epoch: 0.0001019058496846507\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 367\n",
            "Training loss per 0 training steps: 8.718232129467651e-05\n",
            "Training loss epoch: 8.833374946940846e-05\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 368\n",
            "Training loss per 0 training steps: 8.503637945977971e-05\n",
            "Training loss epoch: 8.726268000221656e-05\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 369\n",
            "Training loss per 0 training steps: 6.570528785232455e-05\n",
            "Training loss epoch: 8.644488267843069e-05\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 370\n",
            "Training loss per 0 training steps: 8.472421177430078e-05\n",
            "Training loss epoch: 8.918430467019789e-05\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 371\n",
            "Training loss per 0 training steps: 7.535477925557643e-05\n",
            "Training loss epoch: 8.549478934583021e-05\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 372\n",
            "Training loss per 0 training steps: 0.00010520395881030709\n",
            "Training loss epoch: 9.395872166351182e-05\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 373\n",
            "Training loss per 0 training steps: 9.407498873770237e-05\n",
            "Training loss epoch: 8.810168765194248e-05\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 374\n",
            "Training loss per 0 training steps: 6.396360549842939e-05\n",
            "Training loss epoch: 8.603311713765531e-05\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 375\n",
            "Training loss per 0 training steps: 8.78307328093797e-05\n",
            "Training loss epoch: 8.720505684323143e-05\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 376\n",
            "Training loss per 0 training steps: 8.531806815881282e-05\n",
            "Training loss epoch: 8.1248100589922e-05\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 377\n",
            "Training loss per 0 training steps: 8.863890980137512e-05\n",
            "Training loss epoch: 8.461302847232825e-05\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 378\n",
            "Training loss per 0 training steps: 5.2873696404276416e-05\n",
            "Training loss epoch: 8.675324412858269e-05\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 379\n",
            "Training loss per 0 training steps: 9.004170715343207e-05\n",
            "Training loss epoch: 8.135113815418056e-05\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 380\n",
            "Training loss per 0 training steps: 6.691701855743304e-05\n",
            "Training loss epoch: 8.203224084960918e-05\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 381\n",
            "Training loss per 0 training steps: 9.877762931864709e-05\n",
            "Training loss epoch: 7.844549539489283e-05\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 382\n",
            "Training loss per 0 training steps: 7.244590960908681e-05\n",
            "Training loss epoch: 7.869851409244195e-05\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 383\n",
            "Training loss per 0 training steps: 4.7658228140790015e-05\n",
            "Training loss epoch: 8.707974545056156e-05\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 384\n",
            "Training loss per 0 training steps: 6.364507862599567e-05\n",
            "Training loss epoch: 7.903588236028251e-05\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 385\n",
            "Training loss per 0 training steps: 5.1062525017187e-05\n",
            "Training loss epoch: 7.924541629715047e-05\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 386\n",
            "Training loss per 0 training steps: 6.076411955291405e-05\n",
            "Training loss epoch: 7.671267879535056e-05\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 387\n",
            "Training loss per 0 training steps: 7.488288247259334e-05\n",
            "Training loss epoch: 7.835988041430635e-05\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 388\n",
            "Training loss per 0 training steps: 7.595686474815011e-05\n",
            "Training loss epoch: 7.877008526217348e-05\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 389\n",
            "Training loss per 0 training steps: 5.1818838983308524e-05\n",
            "Training loss epoch: 7.615267046882461e-05\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 390\n",
            "Training loss per 0 training steps: 8.266665099654347e-05\n",
            "Training loss epoch: 7.564137528485541e-05\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 391\n",
            "Training loss per 0 training steps: 7.848340464988723e-05\n",
            "Training loss epoch: 7.858929651168485e-05\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 392\n",
            "Training loss per 0 training steps: 6.597193714696914e-05\n",
            "Training loss epoch: 7.262803198197314e-05\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 393\n",
            "Training loss per 0 training steps: 7.63773059588857e-05\n",
            "Training loss epoch: 7.530751130010079e-05\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 394\n",
            "Training loss per 0 training steps: 7.833925337763503e-05\n",
            "Training loss epoch: 7.281762585383451e-05\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 395\n",
            "Training loss per 0 training steps: 8.293910650536418e-05\n",
            "Training loss epoch: 7.589105704634373e-05\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 396\n",
            "Training loss per 0 training steps: 7.371822721324861e-05\n",
            "Training loss epoch: 7.455793032325649e-05\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 397\n",
            "Training loss per 0 training steps: 0.00010462065984029323\n",
            "Training loss epoch: 7.479071397635077e-05\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 398\n",
            "Training loss per 0 training steps: 6.293570913840085e-05\n",
            "Training loss epoch: 7.446167182934005e-05\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 399\n",
            "Training loss per 0 training steps: 6.936233694432303e-05\n",
            "Training loss epoch: 7.128484139684588e-05\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 400\n",
            "Training loss per 0 training steps: 8.387305570067838e-05\n",
            "Training loss epoch: 7.109134852119799e-05\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 401\n",
            "Training loss per 0 training steps: 7.071023719618097e-05\n",
            "Training loss epoch: 7.293600204623847e-05\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 402\n",
            "Training loss per 0 training steps: 6.891003431519493e-05\n",
            "Training loss epoch: 7.35914139416612e-05\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 403\n",
            "Training loss per 0 training steps: 6.886111805215478e-05\n",
            "Training loss epoch: 6.978125323560865e-05\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 404\n",
            "Training loss per 0 training steps: 7.266562897711992e-05\n",
            "Training loss epoch: 6.968941670493223e-05\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 405\n",
            "Training loss per 0 training steps: 7.678831025259569e-05\n",
            "Training loss epoch: 7.04585321121461e-05\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 406\n",
            "Training loss per 0 training steps: 6.198650953592733e-05\n",
            "Training loss epoch: 6.94776411667893e-05\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 407\n",
            "Training loss per 0 training steps: 7.646733138244599e-05\n",
            "Training loss epoch: 6.807011322962353e-05\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 408\n",
            "Training loss per 0 training steps: 7.273165101651102e-05\n",
            "Training loss epoch: 6.728456264681881e-05\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 409\n",
            "Training loss per 0 training steps: 7.008083775872365e-05\n",
            "Training loss epoch: 6.628771522324921e-05\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 410\n",
            "Training loss per 0 training steps: 6.911868695169687e-05\n",
            "Training loss epoch: 6.667945338752664e-05\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 411\n",
            "Training loss per 0 training steps: 9.147544915322214e-05\n",
            "Training loss epoch: 6.798737861875755e-05\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 412\n",
            "Training loss per 0 training steps: 7.619782263645902e-05\n",
            "Training loss epoch: 0.0019877711647495744\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 413\n",
            "Training loss per 0 training steps: 8.473078196402639e-05\n",
            "Training loss epoch: 0.0001270999709959142\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 414\n",
            "Training loss per 0 training steps: 6.133440183475614e-05\n",
            "Training loss epoch: 7.641885652750109e-05\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 415\n",
            "Training loss per 0 training steps: 6.194464367581531e-05\n",
            "Training loss epoch: 9.194990040365762e-05\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 416\n",
            "Training loss per 0 training steps: 8.080325642367825e-05\n",
            "Training loss epoch: 7.907675474901528e-05\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 417\n",
            "Training loss per 0 training steps: 7.135840860428289e-05\n",
            "Training loss epoch: 7.219779884811335e-05\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 418\n",
            "Training loss per 0 training steps: 7.396468572551385e-05\n",
            "Training loss epoch: 7.184490944685724e-05\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 419\n",
            "Training loss per 0 training steps: 8.314398291986436e-05\n",
            "Training loss epoch: 7.207656199170742e-05\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 420\n",
            "Training loss per 0 training steps: 5.086098826723173e-05\n",
            "Training loss epoch: 6.917139368548912e-05\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 421\n",
            "Training loss per 0 training steps: 0.00016266848251689225\n",
            "Training loss epoch: 7.131104333287415e-05\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 422\n",
            "Training loss per 0 training steps: 8.148603956215084e-05\n",
            "Training loss epoch: 6.809447222622111e-05\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 423\n",
            "Training loss per 0 training steps: 7.556212949566543e-05\n",
            "Training loss epoch: 7.659134947364994e-05\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 424\n",
            "Training loss per 0 training steps: 7.289904897334054e-05\n",
            "Training loss epoch: 6.635894775778677e-05\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 425\n",
            "Training loss per 0 training steps: 6.480321462731808e-05\n",
            "Training loss epoch: 6.624609917101527e-05\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 426\n",
            "Training loss per 0 training steps: 8.904848073143512e-05\n",
            "Training loss epoch: 6.739270065736491e-05\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 427\n",
            "Training loss per 0 training steps: 5.935356966801919e-05\n",
            "Training loss epoch: 6.620196442478725e-05\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 428\n",
            "Training loss per 0 training steps: 4.663981599151157e-05\n",
            "Training loss epoch: 6.581265521769335e-05\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 429\n",
            "Training loss per 0 training steps: 6.89411026542075e-05\n",
            "Training loss epoch: 6.378425829704308e-05\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 430\n",
            "Training loss per 0 training steps: 6.158798350952566e-05\n",
            "Training loss epoch: 6.541190668940544e-05\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 431\n",
            "Training loss per 0 training steps: 6.668204150628299e-05\n",
            "Training loss epoch: 6.23662629853546e-05\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 432\n",
            "Training loss per 0 training steps: 6.457728886744007e-05\n",
            "Training loss epoch: 6.315331302175764e-05\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 433\n",
            "Training loss per 0 training steps: 7.865432417020202e-05\n",
            "Training loss epoch: 6.29088508503628e-05\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 434\n",
            "Training loss per 0 training steps: 5.979917841614224e-05\n",
            "Training loss epoch: 6.299921430278725e-05\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 435\n",
            "Training loss per 0 training steps: 6.702907558064908e-05\n",
            "Training loss epoch: 5.922139825997874e-05\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 436\n",
            "Training loss per 0 training steps: 6.782953278161585e-05\n",
            "Training loss epoch: 6.453480546042556e-05\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 437\n",
            "Training loss per 0 training steps: 7.19256786396727e-05\n",
            "Training loss epoch: 6.0402210086370665e-05\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 438\n",
            "Training loss per 0 training steps: 6.457715790020302e-05\n",
            "Training loss epoch: 5.9823736895244416e-05\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 439\n",
            "Training loss per 0 training steps: 3.9210593968164176e-05\n",
            "Training loss epoch: 6.0401810515031684e-05\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 440\n",
            "Training loss per 0 training steps: 5.379311187425628e-05\n",
            "Training loss epoch: 6.084602470461201e-05\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 441\n",
            "Training loss per 0 training steps: 3.952398765250109e-05\n",
            "Training loss epoch: 5.806487570225727e-05\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 442\n",
            "Training loss per 0 training steps: 5.549665365833789e-05\n",
            "Training loss epoch: 5.896337946372417e-05\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 443\n",
            "Training loss per 0 training steps: 6.245037366170436e-05\n",
            "Training loss epoch: 5.814869291498326e-05\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 444\n",
            "Training loss per 0 training steps: 5.230250098975375e-05\n",
            "Training loss epoch: 5.6153791471539684e-05\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 445\n",
            "Training loss per 0 training steps: 5.828278881381266e-05\n",
            "Training loss epoch: 5.789651277154917e-05\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 446\n",
            "Training loss per 0 training steps: 4.530306614469737e-05\n",
            "Training loss epoch: 6.118290093581891e-05\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 447\n",
            "Training loss per 0 training steps: 5.7904497225536034e-05\n",
            "Training loss epoch: 5.742372468375834e-05\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 448\n",
            "Training loss per 0 training steps: 4.819450987270102e-05\n",
            "Training loss epoch: 5.639891454241782e-05\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 449\n",
            "Training loss per 0 training steps: 5.5621327192056924e-05\n",
            "Training loss epoch: 5.607545123590777e-05\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 450\n",
            "Training loss per 0 training steps: 4.423011705512181e-05\n",
            "Training loss epoch: 5.526754709232288e-05\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 451\n",
            "Training loss per 0 training steps: 4.035126039525494e-05\n",
            "Training loss epoch: 5.5129062881557424e-05\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 452\n",
            "Training loss per 0 training steps: 5.663571573677473e-05\n",
            "Training loss epoch: 5.508392162785943e-05\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 453\n",
            "Training loss per 0 training steps: 4.7186102165142074e-05\n",
            "Training loss epoch: 5.389653385160879e-05\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 454\n",
            "Training loss per 0 training steps: 5.9775578847620636e-05\n",
            "Training loss epoch: 5.494737736929286e-05\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 455\n",
            "Training loss per 0 training steps: 5.7057397498283535e-05\n",
            "Training loss epoch: 5.444463143552033e-05\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 456\n",
            "Training loss per 0 training steps: 4.520064248936251e-05\n",
            "Training loss epoch: 5.250619384848202e-05\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 457\n",
            "Training loss per 0 training steps: 5.897957453271374e-05\n",
            "Training loss epoch: 5.398930170485983e-05\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 458\n",
            "Training loss per 0 training steps: 4.530419028014876e-05\n",
            "Training loss epoch: 5.235022338941538e-05\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 459\n",
            "Training loss per 0 training steps: 4.814657222595997e-05\n",
            "Training loss epoch: 5.299345078431846e-05\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 460\n",
            "Training loss per 0 training steps: 3.549431130522862e-05\n",
            "Training loss epoch: 5.3119922085898e-05\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 461\n",
            "Training loss per 0 training steps: 5.793390664621256e-05\n",
            "Training loss epoch: 5.331710174990197e-05\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 462\n",
            "Training loss per 0 training steps: 5.431476893136278e-05\n",
            "Training loss epoch: 5.4348016116515886e-05\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 463\n",
            "Training loss per 0 training steps: 6.271604797802866e-05\n",
            "Training loss epoch: 5.2018407586729154e-05\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 464\n",
            "Training loss per 0 training steps: 4.8600544687360525e-05\n",
            "Training loss epoch: 0.0005361707677972541\n",
            "Training accuracy epoch: 0.999648382559775\n",
            "Training epoch: 465\n",
            "Training loss per 0 training steps: 7.792517862981185e-05\n",
            "Training loss epoch: 0.0060974818491862\n",
            "Training accuracy epoch: 0.9982638888888888\n",
            "Training epoch: 466\n",
            "Training loss per 0 training steps: 0.03733668848872185\n",
            "Training loss epoch: 0.014202587319838736\n",
            "Training accuracy epoch: 0.9958585777544378\n",
            "Training epoch: 467\n",
            "Training loss per 0 training steps: 0.002461624564602971\n",
            "Training loss epoch: 0.013495579030859517\n",
            "Training accuracy epoch: 0.9975515288015288\n",
            "Training epoch: 468\n",
            "Training loss per 0 training steps: 0.09930432587862015\n",
            "Training loss epoch: 0.013832269765165014\n",
            "Training accuracy epoch: 0.9964198165162261\n",
            "Training epoch: 469\n",
            "Training loss per 0 training steps: 0.00030962959863245487\n",
            "Training loss epoch: 0.0022456665780434073\n",
            "Training accuracy epoch: 0.998898678414097\n",
            "Training epoch: 470\n",
            "Training loss per 0 training steps: 0.0006278450018726289\n",
            "Training loss epoch: 0.0017622496889089234\n",
            "Training accuracy epoch: 0.9993096291722411\n",
            "Training epoch: 471\n",
            "Training loss per 0 training steps: 0.0002795445325318724\n",
            "Training loss epoch: 0.00045202755427453667\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 472\n",
            "Training loss per 0 training steps: 0.00017548928735777736\n",
            "Training loss epoch: 0.00033326125048915856\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 473\n",
            "Training loss per 0 training steps: 0.0001629123871680349\n",
            "Training loss epoch: 0.00025500789100381854\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 474\n",
            "Training loss per 0 training steps: 0.0001380420580971986\n",
            "Training loss epoch: 0.0002240744937201574\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 475\n",
            "Training loss per 0 training steps: 0.00017413684690836817\n",
            "Training loss epoch: 0.00017506236508779693\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 476\n",
            "Training loss per 0 training steps: 0.00010273692896589637\n",
            "Training loss epoch: 0.00017248781477974262\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 477\n",
            "Training loss per 0 training steps: 0.00014192418893799186\n",
            "Training loss epoch: 0.0001618163787497906\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 478\n",
            "Training loss per 0 training steps: 0.0001026298850774765\n",
            "Training loss epoch: 0.00014506839033856522\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 479\n",
            "Training loss per 0 training steps: 9.59386452450417e-05\n",
            "Training loss epoch: 0.002345367585803615\n",
            "Training accuracy epoch: 0.9995791245791246\n",
            "Training epoch: 480\n",
            "Training loss per 0 training steps: 9.017651609610766e-05\n",
            "Training loss epoch: 0.0001887080397864338\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 481\n",
            "Training loss per 0 training steps: 0.0003401346621103585\n",
            "Training loss epoch: 0.00020095596300961915\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 482\n",
            "Training loss per 0 training steps: 0.00036868007737211883\n",
            "Training loss epoch: 0.0001618316139987049\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 483\n",
            "Training loss per 0 training steps: 0.00010435276635689661\n",
            "Training loss epoch: 0.00013916209294014456\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 484\n",
            "Training loss per 0 training steps: 7.097657362464815e-05\n",
            "Training loss epoch: 0.00010879229921556544\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 485\n",
            "Training loss per 0 training steps: 0.00033737363992258906\n",
            "Training loss epoch: 0.00012948958950194842\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 486\n",
            "Training loss per 0 training steps: 7.84805160947144e-05\n",
            "Training loss epoch: 0.00010718765224737581\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 487\n",
            "Training loss per 0 training steps: 0.0001811380498111248\n",
            "Training loss epoch: 0.00010518176956490304\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 488\n",
            "Training loss per 0 training steps: 0.00010212143388343975\n",
            "Training loss epoch: 0.00010296376422047615\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 489\n",
            "Training loss per 0 training steps: 9.761149703990668e-05\n",
            "Training loss epoch: 0.0001027686024220505\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 490\n",
            "Training loss per 0 training steps: 6.890653457958251e-05\n",
            "Training loss epoch: 0.00010625288753847902\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 491\n",
            "Training loss per 0 training steps: 7.603763515362516e-05\n",
            "Training loss epoch: 9.512717254741195e-05\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 492\n",
            "Training loss per 0 training steps: 0.00012111124669900164\n",
            "Training loss epoch: 0.00011149909945136945\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 493\n",
            "Training loss per 0 training steps: 0.00017539982218295336\n",
            "Training loss epoch: 9.534726753675689e-05\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 494\n",
            "Training loss per 0 training steps: 8.960004197433591e-05\n",
            "Training loss epoch: 8.554148826078745e-05\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 495\n",
            "Training loss per 0 training steps: 8.937353413784876e-05\n",
            "Training loss epoch: 9.445083621055043e-05\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 496\n",
            "Training loss per 0 training steps: 6.202509393915534e-05\n",
            "Training loss epoch: 0.00018668580590504766\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 497\n",
            "Training loss per 0 training steps: 0.00011074839858338237\n",
            "Training loss epoch: 8.906046999375879e-05\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 498\n",
            "Training loss per 0 training steps: 7.167218427639455e-05\n",
            "Training loss epoch: 9.130290527537e-05\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 499\n",
            "Training loss per 0 training steps: 7.625760190421715e-05\n",
            "Training loss epoch: 8.776498119307992e-05\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 500\n",
            "Training loss per 0 training steps: 0.00013850002142135054\n",
            "Training loss epoch: 0.00018643917943942748\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 501\n",
            "Training loss per 0 training steps: 6.440205470426008e-05\n",
            "Training loss epoch: 8.132670548851213e-05\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 502\n",
            "Training loss per 0 training steps: 8.065238944254816e-05\n",
            "Training loss epoch: 8.370690011361148e-05\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 503\n",
            "Training loss per 0 training steps: 7.43427881388925e-05\n",
            "Training loss epoch: 7.667559354255597e-05\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 504\n",
            "Training loss per 0 training steps: 7.610811007907614e-05\n",
            "Training loss epoch: 7.633604370009077e-05\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 505\n",
            "Training loss per 0 training steps: 9.263560059480369e-05\n",
            "Training loss epoch: 8.355818560327559e-05\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 506\n",
            "Training loss per 0 training steps: 7.604512211401016e-05\n",
            "Training loss epoch: 7.651288039293529e-05\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 507\n",
            "Training loss per 0 training steps: 0.00011895244824700058\n",
            "Training loss epoch: 7.616602397320094e-05\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 508\n",
            "Training loss per 0 training steps: 6.444840983022004e-05\n",
            "Training loss epoch: 7.362129660274756e-05\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 509\n",
            "Training loss per 0 training steps: 6.365398439811543e-05\n",
            "Training loss epoch: 7.616891495369298e-05\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 510\n",
            "Training loss per 0 training steps: 5.75568265048787e-05\n",
            "Training loss epoch: 7.331769544786464e-05\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 511\n",
            "Training loss per 0 training steps: 5.979826164548285e-05\n",
            "Training loss epoch: 6.851148827990983e-05\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 512\n",
            "Training loss per 0 training steps: 7.593358895974234e-05\n",
            "Training loss epoch: 6.686783702510486e-05\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 513\n",
            "Training loss per 0 training steps: 6.6453154431656e-05\n",
            "Training loss epoch: 6.974202733545098e-05\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 514\n",
            "Training loss per 0 training steps: 4.0678289224160835e-05\n",
            "Training loss epoch: 7.059926156216534e-05\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 515\n",
            "Training loss per 0 training steps: 5.8201021602144465e-05\n",
            "Training loss epoch: 6.606371486365485e-05\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 516\n",
            "Training loss per 0 training steps: 5.6524189858464524e-05\n",
            "Training loss epoch: 6.566697265952826e-05\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 517\n",
            "Training loss per 0 training steps: 7.748701318632811e-05\n",
            "Training loss epoch: 6.602072395859675e-05\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 518\n",
            "Training loss per 0 training steps: 6.642520747845992e-05\n",
            "Training loss epoch: 8.411026859297029e-05\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 519\n",
            "Training loss per 0 training steps: 5.125972893438302e-05\n",
            "Training loss epoch: 7.212345281004673e-05\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 520\n",
            "Training loss per 0 training steps: 6.0940619732718915e-05\n",
            "Training loss epoch: 6.179581002167349e-05\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 521\n",
            "Training loss per 0 training steps: 6.200037023518234e-05\n",
            "Training loss epoch: 6.385129684834585e-05\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 522\n",
            "Training loss per 0 training steps: 6.563137867487967e-05\n",
            "Training loss epoch: 6.300028356539163e-05\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 523\n",
            "Training loss per 0 training steps: 6.240356742637232e-05\n",
            "Training loss epoch: 6.330221791965111e-05\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 524\n",
            "Training loss per 0 training steps: 4.790355160366744e-05\n",
            "Training loss epoch: 6.12880085100187e-05\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 525\n",
            "Training loss per 0 training steps: 4.800509850610979e-05\n",
            "Training loss epoch: 6.80835370682568e-05\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 526\n",
            "Training loss per 0 training steps: 7.139900117181242e-05\n",
            "Training loss epoch: 5.9050611222725515e-05\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 527\n",
            "Training loss per 0 training steps: 6.945302448002622e-05\n",
            "Training loss epoch: 5.7092054703389294e-05\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 528\n",
            "Training loss per 0 training steps: 4.554286351776682e-05\n",
            "Training loss epoch: 5.683287478556546e-05\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 529\n",
            "Training loss per 0 training steps: 6.463191675720736e-05\n",
            "Training loss epoch: 5.821661094766265e-05\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 530\n",
            "Training loss per 0 training steps: 5.613605389953591e-05\n",
            "Training loss epoch: 6.206117692878858e-05\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 531\n",
            "Training loss per 0 training steps: 6.647248665103689e-05\n",
            "Training loss epoch: 5.6075648596258056e-05\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 532\n",
            "Training loss per 0 training steps: 5.900036558159627e-05\n",
            "Training loss epoch: 5.6834564020391554e-05\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 533\n",
            "Training loss per 0 training steps: 5.298948963172734e-05\n",
            "Training loss epoch: 5.5071459428290837e-05\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 534\n",
            "Training loss per 0 training steps: 3.2284169719787315e-05\n",
            "Training loss epoch: 5.430711886826126e-05\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 535\n",
            "Training loss per 0 training steps: 4.4443087972467765e-05\n",
            "Training loss epoch: 5.417077621435359e-05\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 536\n",
            "Training loss per 0 training steps: 4.090123184141703e-05\n",
            "Training loss epoch: 5.497803491986512e-05\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 537\n",
            "Training loss per 0 training steps: 4.145449202042073e-05\n",
            "Training loss epoch: 5.450480057334062e-05\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 538\n",
            "Training loss per 0 training steps: 9.32900802581571e-05\n",
            "Training loss epoch: 5.51914824124348e-05\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 539\n",
            "Training loss per 0 training steps: 4.965711559634656e-05\n",
            "Training loss epoch: 5.8160327777538136e-05\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 540\n",
            "Training loss per 0 training steps: 5.023976700613275e-05\n",
            "Training loss epoch: 5.081597191747278e-05\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 541\n",
            "Training loss per 0 training steps: 4.4651187636191025e-05\n",
            "Training loss epoch: 5.386932813659465e-05\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 542\n",
            "Training loss per 0 training steps: 4.6420067519648e-05\n",
            "Training loss epoch: 5.923777431841396e-05\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 543\n",
            "Training loss per 0 training steps: 6.906181806698442e-05\n",
            "Training loss epoch: 5.183848558469132e-05\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 544\n",
            "Training loss per 0 training steps: 5.212536052567884e-05\n",
            "Training loss epoch: 5.135689298185753e-05\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 545\n",
            "Training loss per 0 training steps: 3.98606680391822e-05\n",
            "Training loss epoch: 5.2133491711477596e-05\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 546\n",
            "Training loss per 0 training steps: 4.15610775235109e-05\n",
            "Training loss epoch: 5.1740550892039515e-05\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 547\n",
            "Training loss per 0 training steps: 4.440998600330204e-05\n",
            "Training loss epoch: 5.080959090264514e-05\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 548\n",
            "Training loss per 0 training steps: 3.977290907641873e-05\n",
            "Training loss epoch: 4.826727157099716e-05\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 549\n",
            "Training loss per 0 training steps: 6.168934487504885e-05\n",
            "Training loss epoch: 4.857725616602693e-05\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 550\n",
            "Training loss per 0 training steps: 3.8907703128643334e-05\n",
            "Training loss epoch: 4.848283242608886e-05\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 551\n",
            "Training loss per 0 training steps: 6.935303099453449e-05\n",
            "Training loss epoch: 4.86204896030055e-05\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 552\n",
            "Training loss per 0 training steps: 3.626299439929426e-05\n",
            "Training loss epoch: 4.628917910546685e-05\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 553\n",
            "Training loss per 0 training steps: 6.329461757559329e-05\n",
            "Training loss epoch: 4.869461175379305e-05\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 554\n",
            "Training loss per 0 training steps: 4.027668546768837e-05\n",
            "Training loss epoch: 4.844085682028284e-05\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 555\n",
            "Training loss per 0 training steps: 4.180076575721614e-05\n",
            "Training loss epoch: 4.7704331033552684e-05\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 556\n",
            "Training loss per 0 training steps: 4.802124749403447e-05\n",
            "Training loss epoch: 4.5092809917453756e-05\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 557\n",
            "Training loss per 0 training steps: 4.915027602692135e-05\n",
            "Training loss epoch: 4.558727414405439e-05\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 558\n",
            "Training loss per 0 training steps: 3.6211484257364646e-05\n",
            "Training loss epoch: 4.7089295549085364e-05\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 559\n",
            "Training loss per 0 training steps: 5.018953015678562e-05\n",
            "Training loss epoch: 4.422820863207259e-05\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 560\n",
            "Training loss per 0 training steps: 5.1417671784292907e-05\n",
            "Training loss epoch: 4.565220054549476e-05\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 561\n",
            "Training loss per 0 training steps: 4.749717118102126e-05\n",
            "Training loss epoch: 4.614257765448807e-05\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 562\n",
            "Training loss per 0 training steps: 4.053445081808604e-05\n",
            "Training loss epoch: 4.474682646105066e-05\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 563\n",
            "Training loss per 0 training steps: 4.5160119043430313e-05\n",
            "Training loss epoch: 4.4461971507795774e-05\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 564\n",
            "Training loss per 0 training steps: 3.599475894588977e-05\n",
            "Training loss epoch: 4.385241193934538e-05\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 565\n",
            "Training loss per 0 training steps: 4.568537406157702e-05\n",
            "Training loss epoch: 4.376994699365847e-05\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 566\n",
            "Training loss per 0 training steps: 4.335712219472043e-05\n",
            "Training loss epoch: 4.289758195833807e-05\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 567\n",
            "Training loss per 0 training steps: 4.1764382331166416e-05\n",
            "Training loss epoch: 4.443909832237599e-05\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 568\n",
            "Training loss per 0 training steps: 3.8339956518029794e-05\n",
            "Training loss epoch: 4.3967905791456964e-05\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 569\n",
            "Training loss per 0 training steps: 2.6365818484919146e-05\n",
            "Training loss epoch: 4.32904948866053e-05\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 570\n",
            "Training loss per 0 training steps: 5.4710191761842e-05\n",
            "Training loss epoch: 4.3019516548762717e-05\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 571\n",
            "Training loss per 0 training steps: 7.406873919535428e-05\n",
            "Training loss epoch: 4.3703552061439645e-05\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 572\n",
            "Training loss per 0 training steps: 6.019531065248884e-05\n",
            "Training loss epoch: 4.2909519379463745e-05\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 573\n",
            "Training loss per 0 training steps: 3.664091127575375e-05\n",
            "Training loss epoch: 4.357051663343251e-05\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 574\n",
            "Training loss per 0 training steps: 4.674074807553552e-05\n",
            "Training loss epoch: 4.2178466477101516e-05\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 575\n",
            "Training loss per 0 training steps: 3.318248491268605e-05\n",
            "Training loss epoch: 4.283805674276664e-05\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 576\n",
            "Training loss per 0 training steps: 7.896590977907181e-05\n",
            "Training loss epoch: 4.502240426518256e-05\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 577\n",
            "Training loss per 0 training steps: 3.474825280136429e-05\n",
            "Training loss epoch: 4.059308351619014e-05\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 578\n",
            "Training loss per 0 training steps: 5.137255720910616e-05\n",
            "Training loss epoch: 4.016705770482076e-05\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 579\n",
            "Training loss per 0 training steps: 3.8271893572527915e-05\n",
            "Training loss epoch: 4.0145494191771526e-05\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 580\n",
            "Training loss per 0 training steps: 4.403437924338505e-05\n",
            "Training loss epoch: 4.1095507943585595e-05\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 581\n",
            "Training loss per 0 training steps: 4.357921352493577e-05\n",
            "Training loss epoch: 4.0203613783281376e-05\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 582\n",
            "Training loss per 0 training steps: 3.7021916796220466e-05\n",
            "Training loss epoch: 4.011031402721225e-05\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 583\n",
            "Training loss per 0 training steps: 3.8025966205168515e-05\n",
            "Training loss epoch: 4.039656141685555e-05\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 584\n",
            "Training loss per 0 training steps: 2.7891939680557698e-05\n",
            "Training loss epoch: 3.865173584927106e-05\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 585\n",
            "Training loss per 0 training steps: 4.244515366735868e-05\n",
            "Training loss epoch: 3.807014718404389e-05\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 586\n",
            "Training loss per 0 training steps: 5.083173527964391e-05\n",
            "Training loss epoch: 3.889365355765525e-05\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 587\n",
            "Training loss per 0 training steps: 3.6203080526320264e-05\n",
            "Training loss epoch: 4.01837990769612e-05\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 588\n",
            "Training loss per 0 training steps: 2.8153712264611386e-05\n",
            "Training loss epoch: 3.9494215798185905e-05\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 589\n",
            "Training loss per 0 training steps: 3.771176125155762e-05\n",
            "Training loss epoch: 4.070492968821782e-05\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 590\n",
            "Training loss per 0 training steps: 3.5196568205719814e-05\n",
            "Training loss epoch: 3.710374721777043e-05\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 591\n",
            "Training loss per 0 training steps: 4.048061964567751e-05\n",
            "Training loss epoch: 3.721621502942677e-05\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 592\n",
            "Training loss per 0 training steps: 3.0428411264438182e-05\n",
            "Training loss epoch: 3.7734889095493905e-05\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 593\n",
            "Training loss per 0 training steps: 3.385091622476466e-05\n",
            "Training loss epoch: 3.705705315345161e-05\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 594\n",
            "Training loss per 0 training steps: 3.9474060031352565e-05\n",
            "Training loss epoch: 3.693248587903023e-05\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 595\n",
            "Training loss per 0 training steps: 4.3306270526954904e-05\n",
            "Training loss epoch: 3.841812849714188e-05\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 596\n",
            "Training loss per 0 training steps: 3.445897527853958e-05\n",
            "Training loss epoch: 3.705411942670859e-05\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 597\n",
            "Training loss per 0 training steps: 3.5524313716450706e-05\n",
            "Training loss epoch: 3.609025210001467e-05\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 598\n",
            "Training loss per 0 training steps: 4.950439324602485e-05\n",
            "Training loss epoch: 3.622371574844389e-05\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 599\n",
            "Training loss per 0 training steps: 2.5570156140020117e-05\n",
            "Training loss epoch: 3.504146328244436e-05\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 600\n",
            "Training loss per 0 training steps: 3.79394696210511e-05\n",
            "Training loss epoch: 3.543548867431431e-05\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 601\n",
            "Training loss per 0 training steps: 2.6140405680052936e-05\n",
            "Training loss epoch: 3.6238979494858846e-05\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 602\n",
            "Training loss per 0 training steps: 3.930556704290211e-05\n",
            "Training loss epoch: 3.501499744137012e-05\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 603\n",
            "Training loss per 0 training steps: 3.0305396649055183e-05\n",
            "Training loss epoch: 3.634440872701816e-05\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 604\n",
            "Training loss per 0 training steps: 3.3649015676928684e-05\n",
            "Training loss epoch: 3.523841451169574e-05\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 605\n",
            "Training loss per 0 training steps: 2.5804210963542573e-05\n",
            "Training loss epoch: 3.43601039579274e-05\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 606\n",
            "Training loss per 0 training steps: 3.226229819119908e-05\n",
            "Training loss epoch: 3.4531385305551034e-05\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 607\n",
            "Training loss per 0 training steps: 4.074405660503544e-05\n",
            "Training loss epoch: 3.556063999591667e-05\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 608\n",
            "Training loss per 0 training steps: 2.2337495465762913e-05\n",
            "Training loss epoch: 3.4889831567852525e-05\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 609\n",
            "Training loss per 0 training steps: 2.7349917218089104e-05\n",
            "Training loss epoch: 3.404412200325169e-05\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 610\n",
            "Training loss per 0 training steps: 3.39266553055495e-05\n",
            "Training loss epoch: 3.459289428064949e-05\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 611\n",
            "Training loss per 0 training steps: 3.039884722966235e-05\n",
            "Training loss epoch: 3.439169980386699e-05\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 612\n",
            "Training loss per 0 training steps: 4.194350549369119e-05\n",
            "Training loss epoch: 3.376571324527807e-05\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 613\n",
            "Training loss per 0 training steps: 2.8964030207134783e-05\n",
            "Training loss epoch: 3.318362875385598e-05\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 614\n",
            "Training loss per 0 training steps: 3.663634925032966e-05\n",
            "Training loss epoch: 3.2602810430641206e-05\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 615\n",
            "Training loss per 0 training steps: 5.2240127843106166e-05\n",
            "Training loss epoch: 3.3165267420069235e-05\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 616\n",
            "Training loss per 0 training steps: 4.0190727304434404e-05\n",
            "Training loss epoch: 3.19565145521968e-05\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 617\n",
            "Training loss per 0 training steps: 2.920409315265715e-05\n",
            "Training loss epoch: 3.221756863543609e-05\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 618\n",
            "Training loss per 0 training steps: 2.530070923967287e-05\n",
            "Training loss epoch: 3.3539703660305044e-05\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 619\n",
            "Training loss per 0 training steps: 3.1427713111042976e-05\n",
            "Training loss epoch: 3.26833949960322e-05\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 620\n",
            "Training loss per 0 training steps: 3.4494321880629286e-05\n",
            "Training loss epoch: 3.3008737773343455e-05\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 621\n",
            "Training loss per 0 training steps: 2.883469824155327e-05\n",
            "Training loss epoch: 3.3503869265890294e-05\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 622\n",
            "Training loss per 0 training steps: 3.971673140767962e-05\n",
            "Training loss epoch: 3.226658282073913e-05\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 623\n",
            "Training loss per 0 training steps: 2.9997101592016406e-05\n",
            "Training loss epoch: 3.3396501748939045e-05\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 624\n",
            "Training loss per 0 training steps: 2.7686948669725098e-05\n",
            "Training loss epoch: 3.19848847235941e-05\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 625\n",
            "Training loss per 0 training steps: 3.480673331068829e-05\n",
            "Training loss epoch: 3.331723261605172e-05\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 626\n",
            "Training loss per 0 training steps: 3.230703805456869e-05\n",
            "Training loss epoch: 3.156137987995559e-05\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 627\n",
            "Training loss per 0 training steps: 3.528868546709418e-05\n",
            "Training loss epoch: 3.070642863652514e-05\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 628\n",
            "Training loss per 0 training steps: 2.8683425625786185e-05\n",
            "Training loss epoch: 3.1031519332221556e-05\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 629\n",
            "Training loss per 0 training steps: 2.03035342565272e-05\n",
            "Training loss epoch: 3.0741491324685434e-05\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 630\n",
            "Training loss per 0 training steps: 3.110806574113667e-05\n",
            "Training loss epoch: 3.2322614212413704e-05\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 631\n",
            "Training loss per 0 training steps: 2.9151822673156857e-05\n",
            "Training loss epoch: 3.048070493605337e-05\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 632\n",
            "Training loss per 0 training steps: 3.0832572520012036e-05\n",
            "Training loss epoch: 3.1737102290207986e-05\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 633\n",
            "Training loss per 0 training steps: 3.223128442186862e-05\n",
            "Training loss epoch: 3.01573566806231e-05\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 634\n",
            "Training loss per 0 training steps: 3.063472831854597e-05\n",
            "Training loss epoch: 3.0256338656424003e-05\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 635\n",
            "Training loss per 0 training steps: 2.7058447813033126e-05\n",
            "Training loss epoch: 3.0321448624211673e-05\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 636\n",
            "Training loss per 0 training steps: 2.4645592930028215e-05\n",
            "Training loss epoch: 2.9451289265125524e-05\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 637\n",
            "Training loss per 0 training steps: 3.468633076408878e-05\n",
            "Training loss epoch: 3.0600787340517854e-05\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 638\n",
            "Training loss per 0 training steps: 3.9868933527031913e-05\n",
            "Training loss epoch: 2.911607907662983e-05\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 639\n",
            "Training loss per 0 training steps: 2.099128505506087e-05\n",
            "Training loss epoch: 2.8664060664596036e-05\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 640\n",
            "Training loss per 0 training steps: 3.434990139794536e-05\n",
            "Training loss epoch: 2.8480611793687178e-05\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 641\n",
            "Training loss per 0 training steps: 3.099730020039715e-05\n",
            "Training loss epoch: 3.3007024740072666e-05\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 642\n",
            "Training loss per 0 training steps: 2.4641607524245046e-05\n",
            "Training loss epoch: 2.863924873963697e-05\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 643\n",
            "Training loss per 0 training steps: 2.0639723516069353e-05\n",
            "Training loss epoch: 2.8990973836092355e-05\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 644\n",
            "Training loss per 0 training steps: 3.210693466826342e-05\n",
            "Training loss epoch: 2.9924296086392133e-05\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 645\n",
            "Training loss per 0 training steps: 2.575462349341251e-05\n",
            "Training loss epoch: 2.9463242450826026e-05\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 646\n",
            "Training loss per 0 training steps: 2.7934707759413868e-05\n",
            "Training loss epoch: 2.778270891212742e-05\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 647\n",
            "Training loss per 0 training steps: 2.543660048104357e-05\n",
            "Training loss epoch: 2.9125431107483262e-05\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 648\n",
            "Training loss per 0 training steps: 3.173912045895122e-05\n",
            "Training loss epoch: 2.902088787474592e-05\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 649\n",
            "Training loss per 0 training steps: 2.964978921227157e-05\n",
            "Training loss epoch: 0.003639266679632177\n",
            "Training accuracy epoch: 0.9996744791666666\n",
            "Training epoch: 650\n",
            "Training loss per 0 training steps: 3.628860940807499e-05\n",
            "Training loss epoch: 4.675497196634145e-05\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 651\n",
            "Training loss per 0 training steps: 7.965436088852584e-05\n",
            "Training loss epoch: 5.640666192145242e-05\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 652\n",
            "Training loss per 0 training steps: 9.219154162565246e-05\n",
            "Training loss epoch: 8.417387319544407e-05\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 653\n",
            "Training loss per 0 training steps: 3.9985927287489176e-05\n",
            "Training loss epoch: 4.098371073268936e-05\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 654\n",
            "Training loss per 0 training steps: 3.162301436532289e-05\n",
            "Training loss epoch: 0.002456801479032341\n",
            "Training accuracy epoch: 0.9996744791666666\n",
            "Training epoch: 655\n",
            "Training loss per 0 training steps: 0.0036470135673880577\n",
            "Training loss epoch: 0.019228153258760965\n",
            "Training accuracy epoch: 0.9919619712501473\n",
            "Training epoch: 656\n",
            "Training loss per 0 training steps: 0.007592030335217714\n",
            "Training loss epoch: 0.011800984474878836\n",
            "Training accuracy epoch: 0.9973284841954023\n",
            "Training epoch: 657\n",
            "Training loss per 0 training steps: 0.0002501286508049816\n",
            "Training loss epoch: 0.025526827144252213\n",
            "Training accuracy epoch: 0.9904033161919831\n",
            "Training epoch: 658\n",
            "Training loss per 0 training steps: 0.000580007501412183\n",
            "Training loss epoch: 0.008352355392465446\n",
            "Training accuracy epoch: 0.9990102049457995\n",
            "Training epoch: 659\n",
            "Training loss per 0 training steps: 0.00030525162583217025\n",
            "Training loss epoch: 0.0020473353542911354\n",
            "Training accuracy epoch: 0.9996744791666666\n",
            "Training epoch: 660\n",
            "Training loss per 0 training steps: 0.0009515075362287462\n",
            "Training loss epoch: 0.0014264644457095226\n",
            "Training accuracy epoch: 0.9996744791666666\n",
            "Training epoch: 661\n",
            "Training loss per 0 training steps: 0.00036452809581533074\n",
            "Training loss epoch: 0.0032326424722365723\n",
            "Training accuracy epoch: 0.9992009943181818\n",
            "Training epoch: 662\n",
            "Training loss per 0 training steps: 0.0028183157555758953\n",
            "Training loss epoch: 0.00344312360236169\n",
            "Training accuracy epoch: 0.999119751021925\n",
            "Training epoch: 663\n",
            "Training loss per 0 training steps: 0.0002497532987035811\n",
            "Training loss epoch: 0.004182051475557576\n",
            "Training accuracy epoch: 0.9993489583333334\n",
            "Training epoch: 664\n",
            "Training loss per 0 training steps: 0.02385740354657173\n",
            "Training loss epoch: 0.00357194225822847\n",
            "Training accuracy epoch: 0.9991333536255412\n",
            "Training epoch: 665\n",
            "Training loss per 0 training steps: 0.0009567526285536587\n",
            "Training loss epoch: 0.0009036198304481028\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 666\n",
            "Training loss per 0 training steps: 0.0004208499740343541\n",
            "Training loss epoch: 0.0003178365932399174\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 667\n",
            "Training loss per 0 training steps: 0.00013370793021749705\n",
            "Training loss epoch: 0.00021128702792339027\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 668\n",
            "Training loss per 0 training steps: 0.00014447656576521695\n",
            "Training loss epoch: 0.002332933900106582\n",
            "Training accuracy epoch: 0.9996744791666666\n",
            "Training epoch: 669\n",
            "Training loss per 0 training steps: 0.00010099846258526668\n",
            "Training loss epoch: 0.0001646423291579898\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 670\n",
            "Training loss per 0 training steps: 0.0004840588371735066\n",
            "Training loss epoch: 0.00015473614985239692\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 671\n",
            "Training loss per 0 training steps: 0.00011638067371677607\n",
            "Training loss epoch: 0.0001403579957089581\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 672\n",
            "Training loss per 0 training steps: 9.761764522409067e-05\n",
            "Training loss epoch: 0.00014649968882925654\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 673\n",
            "Training loss per 0 training steps: 0.00010492347064428031\n",
            "Training loss epoch: 0.0001568729949212866\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 674\n",
            "Training loss per 0 training steps: 0.0001268313790205866\n",
            "Training loss epoch: 0.00012010059739016772\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 675\n",
            "Training loss per 0 training steps: 6.150711851660162e-05\n",
            "Training loss epoch: 0.0001182151360505183\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 676\n",
            "Training loss per 0 training steps: 7.771215314278379e-05\n",
            "Training loss epoch: 0.0001159397697847453\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 677\n",
            "Training loss per 0 training steps: 4.661189086618833e-05\n",
            "Training loss epoch: 0.0001051904688817255\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 678\n",
            "Training loss per 0 training steps: 6.720820238115266e-05\n",
            "Training loss epoch: 0.0001099265036828001\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 679\n",
            "Training loss per 0 training steps: 6.935196870472282e-05\n",
            "Training loss epoch: 9.742276445952787e-05\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 680\n",
            "Training loss per 0 training steps: 9.165875962935388e-05\n",
            "Training loss epoch: 9.57822406538374e-05\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 681\n",
            "Training loss per 0 training steps: 0.00010984965047100559\n",
            "Training loss epoch: 8.9000711038049e-05\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 682\n",
            "Training loss per 0 training steps: 0.00014209259825292975\n",
            "Training loss epoch: 9.269428240562168e-05\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 683\n",
            "Training loss per 0 training steps: 6.296129140537232e-05\n",
            "Training loss epoch: 9.08577861385614e-05\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 684\n",
            "Training loss per 0 training steps: 0.00011174745304742828\n",
            "Training loss epoch: 0.00011300274309178349\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 685\n",
            "Training loss per 0 training steps: 8.164405153365806e-05\n",
            "Training loss epoch: 7.919375639175996e-05\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 686\n",
            "Training loss per 0 training steps: 5.3733092499896884e-05\n",
            "Training loss epoch: 8.810987280109354e-05\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 687\n",
            "Training loss per 0 training steps: 0.00013375999697018415\n",
            "Training loss epoch: 7.869313473444588e-05\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 688\n",
            "Training loss per 0 training steps: 8.529344631824642e-05\n",
            "Training loss epoch: 7.528753879644985e-05\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 689\n",
            "Training loss per 0 training steps: 7.696906686760485e-05\n",
            "Training loss epoch: 7.163904289579175e-05\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 690\n",
            "Training loss per 0 training steps: 8.521133713657036e-05\n",
            "Training loss epoch: 7.190740780060878e-05\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 691\n",
            "Training loss per 0 training steps: 3.930119055439718e-05\n",
            "Training loss epoch: 7.753985634432563e-05\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 692\n",
            "Training loss per 0 training steps: 4.596279177349061e-05\n",
            "Training loss epoch: 7.37885481309301e-05\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 693\n",
            "Training loss per 0 training steps: 0.000107751831819769\n",
            "Training loss epoch: 7.022741616916998e-05\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 694\n",
            "Training loss per 0 training steps: 0.00011349140550009906\n",
            "Training loss epoch: 6.889668808677622e-05\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 695\n",
            "Training loss per 0 training steps: 8.223393524531275e-05\n",
            "Training loss epoch: 6.676590085893015e-05\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 696\n",
            "Training loss per 0 training steps: 5.492633499670774e-05\n",
            "Training loss epoch: 6.39159513108704e-05\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 697\n",
            "Training loss per 0 training steps: 5.643720578518696e-05\n",
            "Training loss epoch: 6.620742260565748e-05\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 698\n",
            "Training loss per 0 training steps: 6.647330883424729e-05\n",
            "Training loss epoch: 6.636365318020883e-05\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 699\n",
            "Training loss per 0 training steps: 6.994914292590693e-05\n",
            "Training loss epoch: 5.939576900952185e-05\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 700\n",
            "Training loss per 0 training steps: 7.354111585300416e-05\n",
            "Training loss epoch: 6.946422066296994e-05\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 701\n",
            "Training loss per 0 training steps: 3.79159209842328e-05\n",
            "Training loss epoch: 6.297916631107607e-05\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 702\n",
            "Training loss per 0 training steps: 9.931871318258345e-05\n",
            "Training loss epoch: 5.9994080250665625e-05\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 703\n",
            "Training loss per 0 training steps: 4.1357179725309834e-05\n",
            "Training loss epoch: 5.885429224387432e-05\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 704\n",
            "Training loss per 0 training steps: 3.092214683420025e-05\n",
            "Training loss epoch: 6.013236058303543e-05\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 705\n",
            "Training loss per 0 training steps: 6.659013888565823e-05\n",
            "Training loss epoch: 5.7673667470226064e-05\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 706\n",
            "Training loss per 0 training steps: 4.780020390171558e-05\n",
            "Training loss epoch: 5.698845567773484e-05\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 707\n",
            "Training loss per 0 training steps: 5.244332714937627e-05\n",
            "Training loss epoch: 5.7070519081510916e-05\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 708\n",
            "Training loss per 0 training steps: 3.70947273040656e-05\n",
            "Training loss epoch: 6.0292711471750714e-05\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 709\n",
            "Training loss per 0 training steps: 5.927415259066038e-05\n",
            "Training loss epoch: 5.623264102420459e-05\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 710\n",
            "Training loss per 0 training steps: 4.2758976633194834e-05\n",
            "Training loss epoch: 5.584379020244038e-05\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 711\n",
            "Training loss per 0 training steps: 5.2174793381709605e-05\n",
            "Training loss epoch: 5.3444334601711795e-05\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 712\n",
            "Training loss per 0 training steps: 7.754704711260274e-05\n",
            "Training loss epoch: 5.340055455841745e-05\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 713\n",
            "Training loss per 0 training steps: 4.9636102630756795e-05\n",
            "Training loss epoch: 5.092694709674106e-05\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 714\n",
            "Training loss per 0 training steps: 5.466950096888468e-05\n",
            "Training loss epoch: 5.4098029674302474e-05\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 715\n",
            "Training loss per 0 training steps: 4.1384009819012135e-05\n",
            "Training loss epoch: 4.90583227777582e-05\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 716\n",
            "Training loss per 0 training steps: 4.028234980069101e-05\n",
            "Training loss epoch: 4.9403660644505486e-05\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 717\n",
            "Training loss per 0 training steps: 4.304023605072871e-05\n",
            "Training loss epoch: 5.273102093876029e-05\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 718\n",
            "Training loss per 0 training steps: 2.8851784009020776e-05\n",
            "Training loss epoch: 5.1542047155332206e-05\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 719\n",
            "Training loss per 0 training steps: 3.9328919228864834e-05\n",
            "Training loss epoch: 4.7346642986667575e-05\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 720\n",
            "Training loss per 0 training steps: 3.825432577286847e-05\n",
            "Training loss epoch: 5.145519889993011e-05\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 721\n",
            "Training loss per 0 training steps: 4.715658360510133e-05\n",
            "Training loss epoch: 4.8458701561078975e-05\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 722\n",
            "Training loss per 0 training steps: 5.620909360004589e-05\n",
            "Training loss epoch: 4.692640398692068e-05\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 723\n",
            "Training loss per 0 training steps: 4.0399238059762865e-05\n",
            "Training loss epoch: 4.701299606798178e-05\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 724\n",
            "Training loss per 0 training steps: 3.60079066012986e-05\n",
            "Training loss epoch: 5.4963866508236e-05\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 725\n",
            "Training loss per 0 training steps: 2.995584873133339e-05\n",
            "Training loss epoch: 4.532999658598177e-05\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 726\n",
            "Training loss per 0 training steps: 7.100529182935134e-05\n",
            "Training loss epoch: 4.6125745787624815e-05\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 727\n",
            "Training loss per 0 training steps: 3.491830648272298e-05\n",
            "Training loss epoch: 4.3928144047337504e-05\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 728\n",
            "Training loss per 0 training steps: 5.937006062595174e-05\n",
            "Training loss epoch: 4.5573876226020125e-05\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 729\n",
            "Training loss per 0 training steps: 4.047447146149352e-05\n",
            "Training loss epoch: 4.594631779279249e-05\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 730\n",
            "Training loss per 0 training steps: 5.78367144044023e-05\n",
            "Training loss epoch: 4.416467042271203e-05\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 731\n",
            "Training loss per 0 training steps: 6.483483593910933e-05\n",
            "Training loss epoch: 4.5597310418088455e-05\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 732\n",
            "Training loss per 0 training steps: 5.886630606255494e-05\n",
            "Training loss epoch: 4.48660172575425e-05\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 733\n",
            "Training loss per 0 training steps: 6.0573242080863565e-05\n",
            "Training loss epoch: 4.289413815664981e-05\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 734\n",
            "Training loss per 0 training steps: 4.6353699872270226e-05\n",
            "Training loss epoch: 4.399919513768206e-05\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 735\n",
            "Training loss per 0 training steps: 0.00015675619943067431\n",
            "Training loss epoch: 5.356776258243675e-05\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 736\n",
            "Training loss per 0 training steps: 5.5503252951893955e-05\n",
            "Training loss epoch: 4.2832441370895445e-05\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 737\n",
            "Training loss per 0 training steps: 3.575757727958262e-05\n",
            "Training loss epoch: 4.2479855589287276e-05\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 738\n",
            "Training loss per 0 training steps: 5.369064456317574e-05\n",
            "Training loss epoch: 4.300901764509035e-05\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 739\n",
            "Training loss per 0 training steps: 4.692063885158859e-05\n",
            "Training loss epoch: 4.233287624325991e-05\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 740\n",
            "Training loss per 0 training steps: 5.0756636483129114e-05\n",
            "Training loss epoch: 4.103542202453051e-05\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 741\n",
            "Training loss per 0 training steps: 3.7499135942198336e-05\n",
            "Training loss epoch: 4.237160965203657e-05\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 742\n",
            "Training loss per 0 training steps: 2.7597934604273178e-05\n",
            "Training loss epoch: 3.998923299756522e-05\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 743\n",
            "Training loss per 0 training steps: 2.6734231141745113e-05\n",
            "Training loss epoch: 4.1554895536440504e-05\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 744\n",
            "Training loss per 0 training steps: 4.198548776912503e-05\n",
            "Training loss epoch: 3.889991512551205e-05\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 745\n",
            "Training loss per 0 training steps: 2.939810474344995e-05\n",
            "Training loss epoch: 4.131418472752557e-05\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 746\n",
            "Training loss per 0 training steps: 3.6406236176844686e-05\n",
            "Training loss epoch: 3.90151556833492e-05\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 747\n",
            "Training loss per 0 training steps: 6.682577804895118e-05\n",
            "Training loss epoch: 3.994696119965132e-05\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 748\n",
            "Training loss per 0 training steps: 3.718012885656208e-05\n",
            "Training loss epoch: 3.757407148441416e-05\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 749\n",
            "Training loss per 0 training steps: 4.767328573507257e-05\n",
            "Training loss epoch: 3.887362208843115e-05\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 750\n",
            "Training loss per 0 training steps: 3.277526775491424e-05\n",
            "Training loss epoch: 3.731770114730656e-05\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 751\n",
            "Training loss per 0 training steps: 3.3546413760632277e-05\n",
            "Training loss epoch: 3.724702946783509e-05\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 752\n",
            "Training loss per 0 training steps: 2.509004480089061e-05\n",
            "Training loss epoch: 3.805229077139908e-05\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 753\n",
            "Training loss per 0 training steps: 4.887104660156183e-05\n",
            "Training loss epoch: 3.776638474543385e-05\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 754\n",
            "Training loss per 0 training steps: 5.639244773192331e-05\n",
            "Training loss epoch: 3.694190748622835e-05\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 755\n",
            "Training loss per 0 training steps: 2.5406332497368567e-05\n",
            "Training loss epoch: 3.620478779945794e-05\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 756\n",
            "Training loss per 0 training steps: 3.617554102675058e-05\n",
            "Training loss epoch: 3.572007138548846e-05\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 757\n",
            "Training loss per 0 training steps: 4.8736081225797534e-05\n",
            "Training loss epoch: 3.6816228809281405e-05\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 758\n",
            "Training loss per 0 training steps: 3.033093707927037e-05\n",
            "Training loss epoch: 3.882225003811376e-05\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 759\n",
            "Training loss per 0 training steps: 4.049152630614117e-05\n",
            "Training loss epoch: 9.000154826329283e-05\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 760\n",
            "Training loss per 0 training steps: 4.124223778489977e-05\n",
            "Training loss epoch: 3.4742582101898734e-05\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 761\n",
            "Training loss per 0 training steps: 3.447782728471793e-05\n",
            "Training loss epoch: 3.655371407755107e-05\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 762\n",
            "Training loss per 0 training steps: 2.819470319082029e-05\n",
            "Training loss epoch: 3.568108953307577e-05\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 763\n",
            "Training loss per 0 training steps: 3.923604162991978e-05\n",
            "Training loss epoch: 3.567842668417143e-05\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 764\n",
            "Training loss per 0 training steps: 3.888941864715889e-05\n",
            "Training loss epoch: 3.4206180619852e-05\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 765\n",
            "Training loss per 0 training steps: 3.013038258359302e-05\n",
            "Training loss epoch: 3.454932812019251e-05\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 766\n",
            "Training loss per 0 training steps: 2.8399250368238427e-05\n",
            "Training loss epoch: 3.45031003234908e-05\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 767\n",
            "Training loss per 0 training steps: 3.9481463318224996e-05\n",
            "Training loss epoch: 3.484339716427106e-05\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 768\n",
            "Training loss per 0 training steps: 2.915981713158544e-05\n",
            "Training loss epoch: 3.4814230275515e-05\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 769\n",
            "Training loss per 0 training steps: 2.4119173758663237e-05\n",
            "Training loss epoch: 4.142982243138249e-05\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 770\n",
            "Training loss per 0 training steps: 3.864846803480759e-05\n",
            "Training loss epoch: 3.522942915878957e-05\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 771\n",
            "Training loss per 0 training steps: 3.3365624403813854e-05\n",
            "Training loss epoch: 3.3388172293295305e-05\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 772\n",
            "Training loss per 0 training steps: 4.070851719006896e-05\n",
            "Training loss epoch: 3.5370275630460433e-05\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 773\n",
            "Training loss per 0 training steps: 4.0955626900540665e-05\n",
            "Training loss epoch: 3.304476649645949e-05\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 774\n",
            "Training loss per 0 training steps: 3.5522902180673555e-05\n",
            "Training loss epoch: 3.360624229268675e-05\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 775\n",
            "Training loss per 0 training steps: 3.738889790838584e-05\n",
            "Training loss epoch: 3.29524086737365e-05\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 776\n",
            "Training loss per 0 training steps: 2.6283740226062946e-05\n",
            "Training loss epoch: 3.2000287925863326e-05\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 777\n",
            "Training loss per 0 training steps: 5.0488742999732494e-05\n",
            "Training loss epoch: 3.383599672209433e-05\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 778\n",
            "Training loss per 0 training steps: 2.222213697677944e-05\n",
            "Training loss epoch: 3.1444396578687396e-05\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 779\n",
            "Training loss per 0 training steps: 4.313235331210308e-05\n",
            "Training loss epoch: 3.3856524927008046e-05\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 780\n",
            "Training loss per 0 training steps: 2.0919786038575694e-05\n",
            "Training loss epoch: 3.2674616250005784e-05\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 781\n",
            "Training loss per 0 training steps: 4.0998806071002036e-05\n",
            "Training loss epoch: 3.2441764991138676e-05\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 782\n",
            "Training loss per 0 training steps: 4.4767835788661614e-05\n",
            "Training loss epoch: 3.099874917703952e-05\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 783\n",
            "Training loss per 0 training steps: 2.424641206744127e-05\n",
            "Training loss epoch: 3.2594626797314653e-05\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 784\n",
            "Training loss per 0 training steps: 3.436250699451193e-05\n",
            "Training loss epoch: 3.2569630244931126e-05\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 785\n",
            "Training loss per 0 training steps: 3.54995318048168e-05\n",
            "Training loss epoch: 3.0935800471828166e-05\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 786\n",
            "Training loss per 0 training steps: 3.096701402682811e-05\n",
            "Training loss epoch: 3.143919487532306e-05\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 787\n",
            "Training loss per 0 training steps: 2.430114909657277e-05\n",
            "Training loss epoch: 2.9985625133122085e-05\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 788\n",
            "Training loss per 0 training steps: 3.374255175003782e-05\n",
            "Training loss epoch: 3.0972563005586075e-05\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 789\n",
            "Training loss per 0 training steps: 3.0023709769011475e-05\n",
            "Training loss epoch: 2.9805927018363338e-05\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 790\n",
            "Training loss per 0 training steps: 2.1778219888801686e-05\n",
            "Training loss epoch: 2.9709909085795516e-05\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 791\n",
            "Training loss per 0 training steps: 2.8101334464736283e-05\n",
            "Training loss epoch: 3.170184118062025e-05\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 792\n",
            "Training loss per 0 training steps: 2.526451316953171e-05\n",
            "Training loss epoch: 2.938909710792359e-05\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 793\n",
            "Training loss per 0 training steps: 2.9083512345096096e-05\n",
            "Training loss epoch: 3.008060806071929e-05\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 794\n",
            "Training loss per 0 training steps: 3.340277908137068e-05\n",
            "Training loss epoch: 4.1351205860943686e-05\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 795\n",
            "Training loss per 0 training steps: 1.82364910870092e-05\n",
            "Training loss epoch: 2.9165492075359605e-05\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 796\n",
            "Training loss per 0 training steps: 2.3827675249776803e-05\n",
            "Training loss epoch: 2.9483182515832596e-05\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 797\n",
            "Training loss per 0 training steps: 2.7014299121219665e-05\n",
            "Training loss epoch: 2.9245612343705336e-05\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 798\n",
            "Training loss per 0 training steps: 3.1271665648091584e-05\n",
            "Training loss epoch: 2.9460979931172915e-05\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 799\n",
            "Training loss per 0 training steps: 2.8650423701037653e-05\n",
            "Training loss epoch: 2.9229970550659345e-05\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 800\n",
            "Training loss per 0 training steps: 3.117924279649742e-05\n",
            "Training loss epoch: 2.9303280674260652e-05\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 801\n",
            "Training loss per 0 training steps: 2.8373000532155856e-05\n",
            "Training loss epoch: 2.83102842028408e-05\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 802\n",
            "Training loss per 0 training steps: 1.951935701072216e-05\n",
            "Training loss epoch: 2.862890823962516e-05\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 803\n",
            "Training loss per 0 training steps: 2.67294544755714e-05\n",
            "Training loss epoch: 2.7854638877518784e-05\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 804\n",
            "Training loss per 0 training steps: 2.9447724955389276e-05\n",
            "Training loss epoch: 2.813279464438286e-05\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 805\n",
            "Training loss per 0 training steps: 2.934209260274656e-05\n",
            "Training loss epoch: 2.7485834380058805e-05\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 806\n",
            "Training loss per 0 training steps: 2.6928328225039877e-05\n",
            "Training loss epoch: 2.8101934276492102e-05\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 807\n",
            "Training loss per 0 training steps: 2.0300069081713445e-05\n",
            "Training loss epoch: 2.8344887444594253e-05\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 808\n",
            "Training loss per 0 training steps: 3.1818821298656985e-05\n",
            "Training loss epoch: 2.7280726802321926e-05\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 809\n",
            "Training loss per 0 training steps: 3.202780499123037e-05\n",
            "Training loss epoch: 2.857066904956203e-05\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 810\n",
            "Training loss per 0 training steps: 1.7904492779052816e-05\n",
            "Training loss epoch: 2.7324303876715323e-05\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 811\n",
            "Training loss per 0 training steps: 2.1840451154275797e-05\n",
            "Training loss epoch: 2.7610399532325875e-05\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 812\n",
            "Training loss per 0 training steps: 2.0748899260070175e-05\n",
            "Training loss epoch: 2.6805598736245884e-05\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 813\n",
            "Training loss per 0 training steps: 2.9254635592224076e-05\n",
            "Training loss epoch: 2.5776442422890494e-05\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 814\n",
            "Training loss per 0 training steps: 2.8997170375077985e-05\n",
            "Training loss epoch: 2.663589399768777e-05\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 815\n",
            "Training loss per 0 training steps: 2.2272812202572823e-05\n",
            "Training loss epoch: 2.750485903864804e-05\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 816\n",
            "Training loss per 0 training steps: 2.3214293833007105e-05\n",
            "Training loss epoch: 2.7145959696402617e-05\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 817\n",
            "Training loss per 0 training steps: 2.316397149115801e-05\n",
            "Training loss epoch: 2.6389968904065125e-05\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 818\n",
            "Training loss per 0 training steps: 3.1092935387277976e-05\n",
            "Training loss epoch: 2.5877579597969696e-05\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 819\n",
            "Training loss per 0 training steps: 2.630415474413894e-05\n",
            "Training loss epoch: 2.709873479034286e-05\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 820\n",
            "Training loss per 0 training steps: 3.373979780008085e-05\n",
            "Training loss epoch: 2.656851969125758e-05\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 821\n",
            "Training loss per 0 training steps: 2.0057867004652508e-05\n",
            "Training loss epoch: 2.552510629053965e-05\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 822\n",
            "Training loss per 0 training steps: 2.813988430716563e-05\n",
            "Training loss epoch: 2.4857777437622037e-05\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 823\n",
            "Training loss per 0 training steps: 1.716440419841092e-05\n",
            "Training loss epoch: 2.5079090240372654e-05\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 824\n",
            "Training loss per 0 training steps: 2.5215154892066494e-05\n",
            "Training loss epoch: 2.5299002724447444e-05\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 825\n",
            "Training loss per 0 training steps: 2.427609979349654e-05\n",
            "Training loss epoch: 2.5467417799518444e-05\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 826\n",
            "Training loss per 0 training steps: 2.4686791221029125e-05\n",
            "Training loss epoch: 2.46790067042942e-05\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 827\n",
            "Training loss per 0 training steps: 2.3491764295613393e-05\n",
            "Training loss epoch: 2.445553028943929e-05\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 828\n",
            "Training loss per 0 training steps: 2.214898086094763e-05\n",
            "Training loss epoch: 2.5949685702168306e-05\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 829\n",
            "Training loss per 0 training steps: 2.4758666768320836e-05\n",
            "Training loss epoch: 2.4870606921467697e-05\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 830\n",
            "Training loss per 0 training steps: 1.9200137103325687e-05\n",
            "Training loss epoch: 2.5447289620691056e-05\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 831\n",
            "Training loss per 0 training steps: 1.557388895889744e-05\n",
            "Training loss epoch: 2.4397240091881638e-05\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 832\n",
            "Training loss per 0 training steps: 2.0672499886131845e-05\n",
            "Training loss epoch: 2.547662696391247e-05\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 833\n",
            "Training loss per 0 training steps: 2.6542502382653765e-05\n",
            "Training loss epoch: 2.437522895585668e-05\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 834\n",
            "Training loss per 0 training steps: 2.7515838155522943e-05\n",
            "Training loss epoch: 2.412147326443422e-05\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 835\n",
            "Training loss per 0 training steps: 1.4342251233756542e-05\n",
            "Training loss epoch: 2.353286921182492e-05\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 836\n",
            "Training loss per 0 training steps: 2.9259836082928814e-05\n",
            "Training loss epoch: 2.3729533874454017e-05\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 837\n",
            "Training loss per 0 training steps: 2.2675747459288687e-05\n",
            "Training loss epoch: 2.4780998804393068e-05\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 838\n",
            "Training loss per 0 training steps: 2.4825005311868154e-05\n",
            "Training loss epoch: 2.3482583628720022e-05\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 839\n",
            "Training loss per 0 training steps: 1.8404365619062446e-05\n",
            "Training loss epoch: 2.309284362430238e-05\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 840\n",
            "Training loss per 0 training steps: 2.3742913981550373e-05\n",
            "Training loss epoch: 2.3343918504300138e-05\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 841\n",
            "Training loss per 0 training steps: 2.1833144273841754e-05\n",
            "Training loss epoch: 2.516793135024879e-05\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 842\n",
            "Training loss per 0 training steps: 2.22868056880543e-05\n",
            "Training loss epoch: 2.4194112332528068e-05\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 843\n",
            "Training loss per 0 training steps: 1.8254870155942626e-05\n",
            "Training loss epoch: 2.370161579771472e-05\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 844\n",
            "Training loss per 0 training steps: 1.9621127648861147e-05\n",
            "Training loss epoch: 2.3259271529241232e-05\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 845\n",
            "Training loss per 0 training steps: 2.3629994757357053e-05\n",
            "Training loss epoch: 2.373470442762482e-05\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 846\n",
            "Training loss per 0 training steps: 2.918937025242485e-05\n",
            "Training loss epoch: 2.3186242894250125e-05\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 847\n",
            "Training loss per 0 training steps: 2.7272215447737835e-05\n",
            "Training loss epoch: 2.270165062630743e-05\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 848\n",
            "Training loss per 0 training steps: 2.486497578502167e-05\n",
            "Training loss epoch: 2.4173238216462778e-05\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 849\n",
            "Training loss per 0 training steps: 2.7128058718517423e-05\n",
            "Training loss epoch: 2.2765523832883144e-05\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 850\n",
            "Training loss per 0 training steps: 2.2724558220943436e-05\n",
            "Training loss epoch: 2.194770096745439e-05\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 851\n",
            "Training loss per 0 training steps: 3.539989847922698e-05\n",
            "Training loss epoch: 2.280500386101873e-05\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 852\n",
            "Training loss per 0 training steps: 3.249735163990408e-05\n",
            "Training loss epoch: 2.253212058652328e-05\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 853\n",
            "Training loss per 0 training steps: 2.9856735636712983e-05\n",
            "Training loss epoch: 2.236239045790474e-05\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 854\n",
            "Training loss per 0 training steps: 1.805934516596608e-05\n",
            "Training loss epoch: 2.2586120849155122e-05\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 855\n",
            "Training loss per 0 training steps: 1.835405419114977e-05\n",
            "Training loss epoch: 2.2337165698142297e-05\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 856\n",
            "Training loss per 0 training steps: 1.6275886082439683e-05\n",
            "Training loss epoch: 2.1676395893640194e-05\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 857\n",
            "Training loss per 0 training steps: 2.423093064862769e-05\n",
            "Training loss epoch: 2.1640073782691616e-05\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 858\n",
            "Training loss per 0 training steps: 2.705027327465359e-05\n",
            "Training loss epoch: 2.1546992002186016e-05\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 859\n",
            "Training loss per 0 training steps: 1.4352024663821794e-05\n",
            "Training loss epoch: 2.124659446659886e-05\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 860\n",
            "Training loss per 0 training steps: 1.916693690873217e-05\n",
            "Training loss epoch: 2.1937005233970314e-05\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 861\n",
            "Training loss per 0 training steps: 2.226374635938555e-05\n",
            "Training loss epoch: 2.0888985242587903e-05\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 862\n",
            "Training loss per 0 training steps: 2.2910615371074528e-05\n",
            "Training loss epoch: 2.1035785114994116e-05\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 863\n",
            "Training loss per 0 training steps: 2.6366455131210387e-05\n",
            "Training loss epoch: 2.1910953061402932e-05\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 864\n",
            "Training loss per 0 training steps: 1.9014258214156143e-05\n",
            "Training loss epoch: 2.1566446321230615e-05\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 865\n",
            "Training loss per 0 training steps: 2.1019284758949652e-05\n",
            "Training loss epoch: 2.1381886654125992e-05\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 866\n",
            "Training loss per 0 training steps: 1.9851915567414835e-05\n",
            "Training loss epoch: 2.1207336052005605e-05\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 867\n",
            "Training loss per 0 training steps: 2.3591222998220474e-05\n",
            "Training loss epoch: 2.130795883203973e-05\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 868\n",
            "Training loss per 0 training steps: 2.096326534228865e-05\n",
            "Training loss epoch: 2.1410235300815355e-05\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 869\n",
            "Training loss per 0 training steps: 2.321406645933166e-05\n",
            "Training loss epoch: 2.0715039454444195e-05\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 870\n",
            "Training loss per 0 training steps: 2.1375633878051303e-05\n",
            "Training loss epoch: 2.1116105396383016e-05\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 871\n",
            "Training loss per 0 training steps: 2.300850610481575e-05\n",
            "Training loss epoch: 2.0644343370198232e-05\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 872\n",
            "Training loss per 0 training steps: 1.4064051356399432e-05\n",
            "Training loss epoch: 1.9909714562042307e-05\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 873\n",
            "Training loss per 0 training steps: 2.1318932340363972e-05\n",
            "Training loss epoch: 2.0310717748846702e-05\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 874\n",
            "Training loss per 0 training steps: 2.1846753952559084e-05\n",
            "Training loss epoch: 2.0250663737897412e-05\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 875\n",
            "Training loss per 0 training steps: 2.019621206272859e-05\n",
            "Training loss epoch: 2.0115608852696216e-05\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 876\n",
            "Training loss per 0 training steps: 1.4779387129237875e-05\n",
            "Training loss epoch: 2.0025579033244867e-05\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 877\n",
            "Training loss per 0 training steps: 2.2171050659380853e-05\n",
            "Training loss epoch: 1.9621167666628025e-05\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 878\n",
            "Training loss per 0 training steps: 2.0479183149291202e-05\n",
            "Training loss epoch: 1.9623582071896333e-05\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 879\n",
            "Training loss per 0 training steps: 2.4242212020908482e-05\n",
            "Training loss epoch: 1.9391570579803858e-05\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 880\n",
            "Training loss per 0 training steps: 1.6381869500037283e-05\n",
            "Training loss epoch: 1.9652743048936827e-05\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 881\n",
            "Training loss per 0 training steps: 2.2022490156814456e-05\n",
            "Training loss epoch: 1.9360475562280044e-05\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 882\n",
            "Training loss per 0 training steps: 2.2541587895830162e-05\n",
            "Training loss epoch: 1.9934240678291342e-05\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 883\n",
            "Training loss per 0 training steps: 1.8762415493256412e-05\n",
            "Training loss epoch: 2.0279786895116558e-05\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 884\n",
            "Training loss per 0 training steps: 2.053341995633673e-05\n",
            "Training loss epoch: 1.9424819432363922e-05\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 885\n",
            "Training loss per 0 training steps: 1.9661898477352224e-05\n",
            "Training loss epoch: 1.9803299589208716e-05\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 886\n",
            "Training loss per 0 training steps: 1.233341663464671e-05\n",
            "Training loss epoch: 1.941118027086001e-05\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 887\n",
            "Training loss per 0 training steps: 1.6919679183047265e-05\n",
            "Training loss epoch: 1.958566334299879e-05\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 888\n",
            "Training loss per 0 training steps: 1.2310598322073929e-05\n",
            "Training loss epoch: 1.8882199431876263e-05\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 889\n",
            "Training loss per 0 training steps: 1.5031242583063431e-05\n",
            "Training loss epoch: 1.8958050077344524e-05\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 890\n",
            "Training loss per 0 training steps: 2.1003848814871162e-05\n",
            "Training loss epoch: 1.970077748107239e-05\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 891\n",
            "Training loss per 0 training steps: 1.5745104974485002e-05\n",
            "Training loss epoch: 1.8727252154349117e-05\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 892\n",
            "Training loss per 0 training steps: 1.5866586181800812e-05\n",
            "Training loss epoch: 1.87142015874997e-05\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 893\n",
            "Training loss per 0 training steps: 1.7127031242125668e-05\n",
            "Training loss epoch: 1.8397512045946012e-05\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 894\n",
            "Training loss per 0 training steps: 1.7277765437029302e-05\n",
            "Training loss epoch: 1.8531031855673064e-05\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 895\n",
            "Training loss per 0 training steps: 1.5003347471065354e-05\n",
            "Training loss epoch: 2.0871857107825537e-05\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 896\n",
            "Training loss per 0 training steps: 1.6881738702068105e-05\n",
            "Training loss epoch: 1.8387053917952773e-05\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 897\n",
            "Training loss per 0 training steps: 1.480480841564713e-05\n",
            "Training loss epoch: 1.812936443457147e-05\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 898\n",
            "Training loss per 0 training steps: 1.691802572167944e-05\n",
            "Training loss epoch: 1.8511058669901104e-05\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 899\n",
            "Training loss per 0 training steps: 1.610502840776462e-05\n",
            "Training loss epoch: 1.8727630200980155e-05\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 900\n",
            "Training loss per 0 training steps: 1.420758508174913e-05\n",
            "Training loss epoch: 1.762201425966244e-05\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 901\n",
            "Training loss per 0 training steps: 2.1218540496192873e-05\n",
            "Training loss epoch: 1.8275127634600114e-05\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 902\n",
            "Training loss per 0 training steps: 2.36351479543373e-05\n",
            "Training loss epoch: 1.9104319941713282e-05\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 903\n",
            "Training loss per 0 training steps: 1.3574083823186811e-05\n",
            "Training loss epoch: 1.8235946223891613e-05\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 904\n",
            "Training loss per 0 training steps: 1.609817991266027e-05\n",
            "Training loss epoch: 1.8001609760176507e-05\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 905\n",
            "Training loss per 0 training steps: 2.4371025574509986e-05\n",
            "Training loss epoch: 1.7763358755473746e-05\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 906\n",
            "Training loss per 0 training steps: 2.1409405235317536e-05\n",
            "Training loss epoch: 1.7737292334156034e-05\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 907\n",
            "Training loss per 0 training steps: 1.4778432159801014e-05\n",
            "Training loss epoch: 1.8226059334362315e-05\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 908\n",
            "Training loss per 0 training steps: 1.719514693832025e-05\n",
            "Training loss epoch: 1.825548815759248e-05\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 909\n",
            "Training loss per 0 training steps: 2.210604361607693e-05\n",
            "Training loss epoch: 1.7074672011100727e-05\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 910\n",
            "Training loss per 0 training steps: 2.034062890743371e-05\n",
            "Training loss epoch: 1.8710335325522465e-05\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 911\n",
            "Training loss per 0 training steps: 2.5342735170852393e-05\n",
            "Training loss epoch: 1.7969138904542586e-05\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 912\n",
            "Training loss per 0 training steps: 1.816524672904052e-05\n",
            "Training loss epoch: 1.761853650350531e-05\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 913\n",
            "Training loss per 0 training steps: 1.4072082194616087e-05\n",
            "Training loss epoch: 1.7006659012016218e-05\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 914\n",
            "Training loss per 0 training steps: 1.8036884284811094e-05\n",
            "Training loss epoch: 1.69137585999124e-05\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 915\n",
            "Training loss per 0 training steps: 2.5437962904106826e-05\n",
            "Training loss epoch: 1.7098145614606135e-05\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 916\n",
            "Training loss per 0 training steps: 2.369137655477971e-05\n",
            "Training loss epoch: 1.711592176434351e-05\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 917\n",
            "Training loss per 0 training steps: 1.160181545856176e-05\n",
            "Training loss epoch: 1.71678694490159e-05\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 918\n",
            "Training loss per 0 training steps: 2.1342537365853786e-05\n",
            "Training loss epoch: 1.702297110265742e-05\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 919\n",
            "Training loss per 0 training steps: 1.641726521484088e-05\n",
            "Training loss epoch: 1.7016735910146963e-05\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 920\n",
            "Training loss per 0 training steps: 2.150124782929197e-05\n",
            "Training loss epoch: 1.6973403262454667e-05\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 921\n",
            "Training loss per 0 training steps: 1.604349927220028e-05\n",
            "Training loss epoch: 1.691085374962616e-05\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 922\n",
            "Training loss per 0 training steps: 1.3400473108049482e-05\n",
            "Training loss epoch: 1.6773020888649626e-05\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 923\n",
            "Training loss per 0 training steps: 2.096384923788719e-05\n",
            "Training loss epoch: 1.681154769054653e-05\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 924\n",
            "Training loss per 0 training steps: 1.4136580830381718e-05\n",
            "Training loss epoch: 1.7391707691179665e-05\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 925\n",
            "Training loss per 0 training steps: 1.8521941456128843e-05\n",
            "Training loss epoch: 1.6611944450536004e-05\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 926\n",
            "Training loss per 0 training steps: 1.456853988202056e-05\n",
            "Training loss epoch: 1.7060247804086732e-05\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 927\n",
            "Training loss per 0 training steps: 1.889411942102015e-05\n",
            "Training loss epoch: 1.6141253505945013e-05\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 928\n",
            "Training loss per 0 training steps: 1.943176903296262e-05\n",
            "Training loss epoch: 1.6074495912713854e-05\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 929\n",
            "Training loss per 0 training steps: 2.2848089429317042e-05\n",
            "Training loss epoch: 1.642536471990752e-05\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 930\n",
            "Training loss per 0 training steps: 1.423823141522007e-05\n",
            "Training loss epoch: 1.5973064516098628e-05\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 931\n",
            "Training loss per 0 training steps: 1.1278705642325804e-05\n",
            "Training loss epoch: 1.65464974391701e-05\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 932\n",
            "Training loss per 0 training steps: 1.557651921757497e-05\n",
            "Training loss epoch: 1.6259552315508092e-05\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 933\n",
            "Training loss per 0 training steps: 1.450161016691709e-05\n",
            "Training loss epoch: 1.5949929471995954e-05\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 934\n",
            "Training loss per 0 training steps: 1.801820326363668e-05\n",
            "Training loss epoch: 1.5666606410983757e-05\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 935\n",
            "Training loss per 0 training steps: 2.5264998839702457e-05\n",
            "Training loss epoch: 1.6210831972784945e-05\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 936\n",
            "Training loss per 0 training steps: 1.5611763956258073e-05\n",
            "Training loss epoch: 1.58703751367284e-05\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 937\n",
            "Training loss per 0 training steps: 1.292238084715791e-05\n",
            "Training loss epoch: 1.582970397369839e-05\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 938\n",
            "Training loss per 0 training steps: 1.342600353382295e-05\n",
            "Training loss epoch: 1.562379081102942e-05\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 939\n",
            "Training loss per 0 training steps: 1.4068674317968544e-05\n",
            "Training loss epoch: 1.547722717987199e-05\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 940\n",
            "Training loss per 0 training steps: 1.2942791727255099e-05\n",
            "Training loss epoch: 1.564424815114762e-05\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 941\n",
            "Training loss per 0 training steps: 1.636426713957917e-05\n",
            "Training loss epoch: 1.53553078234836e-05\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 942\n",
            "Training loss per 0 training steps: 1.4469284906226676e-05\n",
            "Training loss epoch: 1.524228863066431e-05\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 943\n",
            "Training loss per 0 training steps: 2.678153214219492e-05\n",
            "Training loss epoch: 1.5780207225664828e-05\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 944\n",
            "Training loss per 0 training steps: 1.2115388926758897e-05\n",
            "Training loss epoch: 1.5281121325945907e-05\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 945\n",
            "Training loss per 0 training steps: 1.3926696738053579e-05\n",
            "Training loss epoch: 1.5030153690531733e-05\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 946\n",
            "Training loss per 0 training steps: 1.2632581274374388e-05\n",
            "Training loss epoch: 1.5058023943007962e-05\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 947\n",
            "Training loss per 0 training steps: 1.609844184713438e-05\n",
            "Training loss epoch: 1.4902080541408699e-05\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 948\n",
            "Training loss per 0 training steps: 1.690494900685735e-05\n",
            "Training loss epoch: 1.5186602695393958e-05\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 949\n",
            "Training loss per 0 training steps: 1.6416461221524514e-05\n",
            "Training loss epoch: 1.558425378789252e-05\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 950\n",
            "Training loss per 0 training steps: 2.2472562704933807e-05\n",
            "Training loss epoch: 1.4669056857504378e-05\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 951\n",
            "Training loss per 0 training steps: 1.4434197510126978e-05\n",
            "Training loss epoch: 1.4845029985129562e-05\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 952\n",
            "Training loss per 0 training steps: 1.2763094673573505e-05\n",
            "Training loss epoch: 1.4567379063616196e-05\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 953\n",
            "Training loss per 0 training steps: 1.5382725905510597e-05\n",
            "Training loss epoch: 1.4871625732363706e-05\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 954\n",
            "Training loss per 0 training steps: 1.5982748664100654e-05\n",
            "Training loss epoch: 1.4518243915517814e-05\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 955\n",
            "Training loss per 0 training steps: 1.6713922377675772e-05\n",
            "Training loss epoch: 1.4834534188897427e-05\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 956\n",
            "Training loss per 0 training steps: 9.366260201204568e-06\n",
            "Training loss epoch: 1.4566248561701892e-05\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 957\n",
            "Training loss per 0 training steps: 1.2342981790425256e-05\n",
            "Training loss epoch: 1.4574323055664232e-05\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 958\n",
            "Training loss per 0 training steps: 1.2900026376883034e-05\n",
            "Training loss epoch: 1.4657095258977884e-05\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 959\n",
            "Training loss per 0 training steps: 1.7683463738649152e-05\n",
            "Training loss epoch: 1.4535894933942473e-05\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 960\n",
            "Training loss per 0 training steps: 1.344611791864736e-05\n",
            "Training loss epoch: 1.5015940486288551e-05\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 961\n",
            "Training loss per 0 training steps: 1.565274033055175e-05\n",
            "Training loss epoch: 1.4178375598324541e-05\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 962\n",
            "Training loss per 0 training steps: 8.808870916254818e-06\n",
            "Training loss epoch: 1.427981336140268e-05\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 963\n",
            "Training loss per 0 training steps: 1.662593422224745e-05\n",
            "Training loss epoch: 1.435985147206035e-05\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 964\n",
            "Training loss per 0 training steps: 1.065687229129253e-05\n",
            "Training loss epoch: 1.4106479284237139e-05\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 965\n",
            "Training loss per 0 training steps: 1.3389788364293054e-05\n",
            "Training loss epoch: 1.46303634664946e-05\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 966\n",
            "Training loss per 0 training steps: 1.505612181063043e-05\n",
            "Training loss epoch: 1.3490312994690612e-05\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 967\n",
            "Training loss per 0 training steps: 1.566592800372746e-05\n",
            "Training loss epoch: 1.4188337975914086e-05\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 968\n",
            "Training loss per 0 training steps: 1.529241490061395e-05\n",
            "Training loss epoch: 1.3592892173619475e-05\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 969\n",
            "Training loss per 0 training steps: 1.5466930562979542e-05\n",
            "Training loss epoch: 1.4189850086647008e-05\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 970\n",
            "Training loss per 0 training steps: 1.0028348697233014e-05\n",
            "Training loss epoch: 1.3321742017069482e-05\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 971\n",
            "Training loss per 0 training steps: 1.4639634173363447e-05\n",
            "Training loss epoch: 1.3454932817088169e-05\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 972\n",
            "Training loss per 0 training steps: 1.2729516129184049e-05\n",
            "Training loss epoch: 1.3723376620570585e-05\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 973\n",
            "Training loss per 0 training steps: 1.4354650375025813e-05\n",
            "Training loss epoch: 1.3763225676181415e-05\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 974\n",
            "Training loss per 0 training steps: 1.3108423445373774e-05\n",
            "Training loss epoch: 1.431561281606264e-05\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 975\n",
            "Training loss per 0 training steps: 1.4916130567144137e-05\n",
            "Training loss epoch: 1.3363899370233412e-05\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 976\n",
            "Training loss per 0 training steps: 1.0588546501821838e-05\n",
            "Training loss epoch: 1.3361578794501838e-05\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 977\n",
            "Training loss per 0 training steps: 1.7242960893781856e-05\n",
            "Training loss epoch: 1.320236045406394e-05\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 978\n",
            "Training loss per 0 training steps: 1.09423363028327e-05\n",
            "Training loss epoch: 1.296313697215131e-05\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 979\n",
            "Training loss per 0 training steps: 1.4323511095426511e-05\n",
            "Training loss epoch: 1.3277201333039557e-05\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 980\n",
            "Training loss per 0 training steps: 1.8757218640530482e-05\n",
            "Training loss epoch: 1.3320013219223862e-05\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 981\n",
            "Training loss per 0 training steps: 1.0280263268214185e-05\n",
            "Training loss epoch: 1.337151949580099e-05\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 982\n",
            "Training loss per 0 training steps: 8.996024916996248e-06\n",
            "Training loss epoch: 1.3457635607968163e-05\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 983\n",
            "Training loss per 0 training steps: 1.8438688130117953e-05\n",
            "Training loss epoch: 1.263078919085577e-05\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 984\n",
            "Training loss per 0 training steps: 1.1705155884556007e-05\n",
            "Training loss epoch: 1.2907511366696175e-05\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 985\n",
            "Training loss per 0 training steps: 1.052521383826388e-05\n",
            "Training loss epoch: 1.3309505372186928e-05\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 986\n",
            "Training loss per 0 training steps: 1.0907486284850165e-05\n",
            "Training loss epoch: 1.2713795513263904e-05\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 987\n",
            "Training loss per 0 training steps: 9.997874258260708e-06\n",
            "Training loss epoch: 1.2744030830920868e-05\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 988\n",
            "Training loss per 0 training steps: 1.062763749359874e-05\n",
            "Training loss epoch: 1.2548801350931171e-05\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 989\n",
            "Training loss per 0 training steps: 1.4398796338355169e-05\n",
            "Training loss epoch: 1.3267362722520678e-05\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 990\n",
            "Training loss per 0 training steps: 1.2805412552552298e-05\n",
            "Training loss epoch: 1.2897383461070907e-05\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 991\n",
            "Training loss per 0 training steps: 1.145136047853157e-05\n",
            "Training loss epoch: 1.2988478905147835e-05\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 992\n",
            "Training loss per 0 training steps: 1.0938982995867264e-05\n",
            "Training loss epoch: 1.2315421827224782e-05\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 993\n",
            "Training loss per 0 training steps: 1.3178312656236812e-05\n",
            "Training loss epoch: 1.2288165104716123e-05\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 994\n",
            "Training loss per 0 training steps: 1.4295684195531067e-05\n",
            "Training loss epoch: 1.2522563110906049e-05\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 995\n",
            "Training loss per 0 training steps: 9.637182301958092e-06\n",
            "Training loss epoch: 1.2399045014414392e-05\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 996\n",
            "Training loss per 0 training steps: 1.2815081390726846e-05\n",
            "Training loss epoch: 1.2983094544930887e-05\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 997\n",
            "Training loss per 0 training steps: 1.195190543512581e-05\n",
            "Training loss epoch: 1.2270857382645772e-05\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 998\n",
            "Training loss per 0 training steps: 1.433478610124439e-05\n",
            "Training loss epoch: 1.2268589368128838e-05\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 999\n",
            "Training loss per 0 training steps: 1.4205991647031624e-05\n",
            "Training loss epoch: 1.2269756552996114e-05\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 1000\n",
            "Training loss per 0 training steps: 1.474019154557027e-05\n",
            "Training loss epoch: 1.211147408260634e-05\n",
            "Training accuracy epoch: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model Validation"
      ],
      "metadata": {
        "id": "hUDclDM3FEKt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def validation(model, testing_loader):\n",
        "    # put model in evaluation mode\n",
        "    model.eval()\n",
        "    \n",
        "    eval_loss, eval_accuracy = 0, 0\n",
        "    nb_eval_examples, nb_eval_steps = 0, 0\n",
        "    eval_preds, eval_labels = [], []\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for idx, batch in enumerate(testing_loader):\n",
        "            \n",
        "            ids = batch['ids'].to(device, dtype = torch.long)\n",
        "            mask = batch['mask'].to(device, dtype = torch.long)\n",
        "            targets = batch['targets'].to(device, dtype = torch.long)\n",
        "            \n",
        "            outputs = model(input_ids=ids, attention_mask=mask, labels=targets)\n",
        "            loss, eval_logits = outputs.loss, outputs.logits\n",
        "            \n",
        "            eval_loss += loss.item()\n",
        "\n",
        "            nb_eval_steps += 1\n",
        "            nb_eval_examples += targets.size(0)\n",
        "        \n",
        "            if idx % 100==0:\n",
        "                loss_step = eval_loss/nb_eval_steps\n",
        "                print(f\"Validation loss per {idx} evaluation steps: {loss_step}\")\n",
        "              \n",
        "            # compute evaluation accuracy\n",
        "            flattened_targets = targets.view(-1) # shape (batch_size * seq_len,)\n",
        "            active_logits = eval_logits.view(-1, model.num_labels) # shape (batch_size * seq_len, num_labels)\n",
        "            flattened_predictions = torch.argmax(active_logits, axis=1) # shape (batch_size * seq_len,)\n",
        "            # now, use mask to determine where we should compare predictions with targets (includes [CLS] and [SEP] token predictions)\n",
        "            active_accuracy = mask.view(-1) == 1 # active accuracy is also of shape (batch_size * seq_len,)\n",
        "            targets = torch.masked_select(flattened_targets, active_accuracy)\n",
        "            predictions = torch.masked_select(flattened_predictions, active_accuracy)\n",
        "            \n",
        "            eval_labels.extend(targets)\n",
        "            eval_preds.extend(predictions)\n",
        "            \n",
        "            tmp_eval_accuracy = accuracy_score(targets.cpu().numpy(), predictions.cpu().numpy())\n",
        "            eval_accuracy += tmp_eval_accuracy\n",
        "    \n",
        "    #print(eval_labels)\n",
        "    #print(eval_preds)\n",
        "\n",
        "    labels = [id2label[id.item()] for id in eval_labels]\n",
        "    predictions = [id2label[id.item()] for id in eval_preds]\n",
        "\n",
        "    #print(labels)\n",
        "    #print(predictions)\n",
        "    \n",
        "    eval_loss = eval_loss / nb_eval_steps\n",
        "    eval_accuracy = eval_accuracy / nb_eval_steps\n",
        "    print(f\"Validation Loss: {eval_loss}\")\n",
        "    print(f\"Validation Accuracy: {eval_accuracy}\")\n",
        "\n",
        "    return labels, predictions"
      ],
      "metadata": {
        "id": "d-M5ePQL5b8z"
      },
      "execution_count": 176,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "labels, predictions = validation(model, testing_loader)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0iIDLasnFHEf",
        "outputId": "5b8b96ad-6e68-4a4b-c869-eb544fec2e08"
      },
      "execution_count": 177,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation loss per 0 evaluation steps: 1.8650262063601986e-05\n",
            "Validation Loss: 0.48969188373087036\n",
            "Validation Accuracy: 0.9354299532312925\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from seqeval.metrics import classification_report\n",
        "# print(labels)\n",
        "# print(predictions)\n",
        "\n",
        "print(classification_report([labels], [predictions]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XujcYEqtHrhz",
        "outputId": "98627f84-cfe2-4868-b648-cb0851d65482"
      },
      "execution_count": 178,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                  precision    recall  f1-score   support\n",
            "\n",
            "MedicalCondition       0.00      0.00      0.00         1\n",
            "        Medicine       0.60      0.44      0.51        57\n",
            "        Pathogen       0.50      0.45      0.48        11\n",
            "\n",
            "       micro avg       0.58      0.43      0.50        69\n",
            "       macro avg       0.37      0.30      0.33        69\n",
            "    weighted avg       0.57      0.43      0.49        69\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Inference"
      ],
      "metadata": {
        "id": "4GxCOLTbFLGw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sentence = \"I have malaria and my friend has a common cold.\"\n",
        "\n",
        "inputs = tokenizer(sentence, padding='max_length', truncation=True, max_length=MAX_LEN, return_tensors=\"pt\")\n",
        "\n",
        "# move to gpu\n",
        "ids = inputs[\"input_ids\"].to(device)\n",
        "mask = inputs[\"attention_mask\"].to(device)\n",
        "# forward pass\n",
        "outputs = model(ids, mask)\n",
        "logits = outputs[0]\n",
        "\n",
        "active_logits = logits.view(-1, model.num_labels) # shape (batch_size * seq_len, num_labels)\n",
        "flattened_predictions = torch.argmax(active_logits, axis=1) # shape (batch_size*seq_len,) - predictions at the token level\n",
        "\n",
        "tokens = tokenizer.convert_ids_to_tokens(ids.squeeze().tolist())\n",
        "token_predictions = [id2label[i] for i in flattened_predictions.cpu().numpy()]\n",
        "wp_preds = list(zip(tokens, token_predictions)) # list of tuples. Each tuple = (wordpiece, prediction)\n",
        "\n",
        "word_level_predictions = []\n",
        "for pair in wp_preds:\n",
        "  if (pair[0].startswith(\" ##\")) or (pair[0] in ['[CLS]', '[SEP]', '[PAD]']):\n",
        "    # skip prediction\n",
        "    continue\n",
        "  else:\n",
        "    word_level_predictions.append(pair[1])\n",
        "\n",
        "# we join tokens, if they are not special ones\n",
        "str_rep = \" \".join([t[0] for t in wp_preds if t[0] not in ['[CLS]', '[SEP]', '[PAD]']]).replace(\" ##\", \"\")\n",
        "print(str_rep)\n",
        "print(word_level_predictions)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uRt6Q39JFHG6",
        "outputId": "0b611d13-8db8-4f93-d9ce-f5271dcf03c8"
      },
      "execution_count": 180,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "i have malaria and my friend has a common cold .\n",
            "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "lig_SbPQFHJv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "2IGCrZv1FHVf"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}